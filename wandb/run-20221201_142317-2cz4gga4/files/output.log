Use Adam
Start training epoch: (0/100)
Epoch (0), Batch(0/2189), loss: 14.192142, imid loss: 5.323435, cmid loss: 8.868708
/home/zht/anaconda3/envs/py38torch19/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/zht/anaconda3/envs/py38torch19/lib/python3.8/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
Epoch (0), Batch(200/2189), loss: 6.513151, imid loss: 2.928687, cmid loss: 3.584465
Epoch (0), Batch(400/2189), loss: 6.011841, imid loss: 2.660301, cmid loss: 3.351541
Epoch (0), Batch(600/2189), loss: 5.655104, imid loss: 2.475030, cmid loss: 3.180074
Epoch (0), Batch(800/2189), loss: 5.344276, imid loss: 2.314072, cmid loss: 3.030204
Epoch (0), Batch(1000/2189), loss: 5.068765, imid loss: 2.165084, cmid loss: 2.903680
Epoch (0), Batch(1200/2189), loss: 4.843163, imid loss: 2.046842, cmid loss: 2.796321
Epoch (0), Batch(1400/2189), loss: 4.634077, imid loss: 1.922033, cmid loss: 2.712044
Epoch (0), Batch(1600/2189), loss: 4.453301, imid loss: 1.812258, cmid loss: 2.641043
Epoch (0), Batch(1800/2189), loss: 4.307249, imid loss: 1.722817, cmid loss: 2.584432
Epoch (0), Batch(2000/2189), loss: 4.162688, imid loss: 1.644479, cmid loss: 2.518209
Train 0, loss: 4.067812
Linear Accuracy : 0.8788492706645057
==> Saving Best Model...
==> Saving...
Start training epoch: (1/100)
Epoch (1), Batch(0/2189), loss: 2.254162, imid loss: 0.728978, cmid loss: 1.525185
Epoch (1), Batch(200/2189), loss: 2.581181, imid loss: 0.816952, cmid loss: 1.764229
Epoch (1), Batch(400/2189), loss: 2.539097, imid loss: 0.797363, cmid loss: 1.741734
Epoch (1), Batch(600/2189), loss: 2.493443, imid loss: 0.774800, cmid loss: 1.718642
Epoch (1), Batch(800/2189), loss: 2.449016, imid loss: 0.752023, cmid loss: 1.696993
Epoch (1), Batch(1000/2189), loss: 2.413816, imid loss: 0.737724, cmid loss: 1.676092
Epoch (1), Batch(1200/2189), loss: 2.366923, imid loss: 0.717671, cmid loss: 1.649252
Epoch (1), Batch(1400/2189), loss: 2.330557, imid loss: 0.704309, cmid loss: 1.626248
Epoch (1), Batch(1600/2189), loss: 2.288562, imid loss: 0.685590, cmid loss: 1.602972
Epoch (1), Batch(1800/2189), loss: 2.258364, imid loss: 0.670383, cmid loss: 1.587981
Epoch (1), Batch(2000/2189), loss: 2.218553, imid loss: 0.654584, cmid loss: 1.563970
Train 1, loss: 2.191649
Linear Accuracy : 0.8889789303079416
==> Saving Best Model...
Start training epoch: (2/100)
Epoch (2), Batch(0/2189), loss: 2.046612, imid loss: 0.549842, cmid loss: 1.496770
Epoch (2), Batch(200/2189), loss: 1.802649, imid loss: 0.500918, cmid loss: 1.301731
Epoch (2), Batch(400/2189), loss: 1.804093, imid loss: 0.501209, cmid loss: 1.302884
Epoch (2), Batch(600/2189), loss: 1.754517, imid loss: 0.478866, cmid loss: 1.275651
Epoch (2), Batch(800/2189), loss: 1.735162, imid loss: 0.472733, cmid loss: 1.262429
Epoch (2), Batch(1000/2189), loss: 1.706676, imid loss: 0.464315, cmid loss: 1.242361
Epoch (2), Batch(1200/2189), loss: 1.687696, imid loss: 0.460929, cmid loss: 1.226767
Epoch (2), Batch(1400/2189), loss: 1.666810, imid loss: 0.454617, cmid loss: 1.212193
Epoch (2), Batch(1600/2189), loss: 1.650580, imid loss: 0.449104, cmid loss: 1.201476
Epoch (2), Batch(1800/2189), loss: 1.635377, imid loss: 0.444956, cmid loss: 1.190421
Epoch (2), Batch(2000/2189), loss: 1.615747, imid loss: 0.438497, cmid loss: 1.177251
Train 2, loss: 1.598132
Linear Accuracy : 0.8845218800648298
Start training epoch: (3/100)
Epoch (3), Batch(0/2189), loss: 1.327157, imid loss: 0.364683, cmid loss: 0.962474
Epoch (3), Batch(200/2189), loss: 1.409405, imid loss: 0.381814, cmid loss: 1.027591
Epoch (3), Batch(400/2189), loss: 1.372420, imid loss: 0.364385, cmid loss: 1.008035
Epoch (3), Batch(600/2189), loss: 1.355093, imid loss: 0.354847, cmid loss: 1.000245
Epoch (3), Batch(800/2189), loss: 1.358091, imid loss: 0.356775, cmid loss: 1.001315
Epoch (3), Batch(1000/2189), loss: 1.335288, imid loss: 0.351579, cmid loss: 0.983708
Epoch (3), Batch(1200/2189), loss: 1.328328, imid loss: 0.348621, cmid loss: 0.979707
Epoch (3), Batch(1400/2189), loss: 1.316687, imid loss: 0.345805, cmid loss: 0.970882
Epoch (3), Batch(1600/2189), loss: 1.310202, imid loss: 0.344504, cmid loss: 0.965698
Epoch (3), Batch(1800/2189), loss: 1.299531, imid loss: 0.341536, cmid loss: 0.957995
Epoch (3), Batch(2000/2189), loss: 1.291478, imid loss: 0.338996, cmid loss: 0.952482
Train 3, loss: 1.283862
Linear Accuracy : 0.8889789303079416
Start training epoch: (4/100)
Epoch (4), Batch(0/2189), loss: 1.275617, imid loss: 0.275262, cmid loss: 1.000355
Epoch (4), Batch(200/2189), loss: 1.165526, imid loss: 0.297536, cmid loss: 0.867989
Epoch (4), Batch(400/2189), loss: 1.154019, imid loss: 0.301249, cmid loss: 0.852770
Epoch (4), Batch(600/2189), loss: 1.131512, imid loss: 0.291400, cmid loss: 0.840112
Epoch (4), Batch(800/2189), loss: 1.118420, imid loss: 0.290726, cmid loss: 0.827694
Epoch (4), Batch(1000/2189), loss: 1.115673, imid loss: 0.290619, cmid loss: 0.825054
Epoch (4), Batch(1200/2189), loss: 1.111998, imid loss: 0.288396, cmid loss: 0.823601
Epoch (4), Batch(1400/2189), loss: 1.102839, imid loss: 0.285016, cmid loss: 0.817823
Epoch (4), Batch(1600/2189), loss: 1.103257, imid loss: 0.286288, cmid loss: 0.816969
Epoch (4), Batch(1800/2189), loss: 1.097433, imid loss: 0.286076, cmid loss: 0.811357
Epoch (4), Batch(2000/2189), loss: 1.090447, imid loss: 0.283016, cmid loss: 0.807431
Train 4, loss: 1.083758
Linear Accuracy : 0.9015397082658023
==> Saving Best Model...
Start training epoch: (5/100)
Epoch (5), Batch(0/2189), loss: 0.850022, imid loss: 0.325099, cmid loss: 0.524923
Epoch (5), Batch(200/2189), loss: 1.009032, imid loss: 0.254448, cmid loss: 0.754584
Epoch (5), Batch(400/2189), loss: 0.994905, imid loss: 0.256358, cmid loss: 0.738546
Epoch (5), Batch(600/2189), loss: 0.972976, imid loss: 0.250675, cmid loss: 0.722301
Epoch (5), Batch(800/2189), loss: 0.978658, imid loss: 0.250779, cmid loss: 0.727879
Epoch (5), Batch(1000/2189), loss: 0.969632, imid loss: 0.247904, cmid loss: 0.721728
Epoch (5), Batch(1200/2189), loss: 0.973779, imid loss: 0.249012, cmid loss: 0.724767
Epoch (5), Batch(1400/2189), loss: 0.965272, imid loss: 0.246350, cmid loss: 0.718922
Epoch (5), Batch(1600/2189), loss: 0.966246, imid loss: 0.247056, cmid loss: 0.719190
Epoch (5), Batch(1800/2189), loss: 0.962114, imid loss: 0.245864, cmid loss: 0.716250
Epoch (5), Batch(2000/2189), loss: 0.960336, imid loss: 0.245203, cmid loss: 0.715133
Train 5, loss: 0.957765
Linear Accuracy : 0.8910048622366289
==> Saving...
Start training epoch: (6/100)
Epoch (6), Batch(0/2189), loss: 1.006942, imid loss: 0.231702, cmid loss: 0.775240
Epoch (6), Batch(200/2189), loss: 0.905997, imid loss: 0.240790, cmid loss: 0.665207
Epoch (6), Batch(400/2189), loss: 0.891614, imid loss: 0.231630, cmid loss: 0.659983
Epoch (6), Batch(600/2189), loss: 0.888063, imid loss: 0.229588, cmid loss: 0.658475
Epoch (6), Batch(800/2189), loss: 0.883703, imid loss: 0.229730, cmid loss: 0.653973
Epoch (6), Batch(1000/2189), loss: 0.875316, imid loss: 0.227257, cmid loss: 0.648058
Epoch (6), Batch(1200/2189), loss: 0.874233, imid loss: 0.226159, cmid loss: 0.648073
Epoch (6), Batch(1400/2189), loss: 0.869668, imid loss: 0.224933, cmid loss: 0.644735
Epoch (6), Batch(1600/2189), loss: 0.866383, imid loss: 0.222577, cmid loss: 0.643806
Epoch (6), Batch(1800/2189), loss: 0.861981, imid loss: 0.221921, cmid loss: 0.640061
Epoch (6), Batch(2000/2189), loss: 0.859757, imid loss: 0.221009, cmid loss: 0.638748
Train 6, loss: 0.856624
Linear Accuracy : 0.8841166936790924
Start training epoch: (7/100)
Epoch (7), Batch(0/2189), loss: 0.377141, imid loss: 0.092677, cmid loss: 0.284464
Epoch (7), Batch(200/2189), loss: 0.801154, imid loss: 0.205414, cmid loss: 0.595741
Epoch (7), Batch(400/2189), loss: 0.798979, imid loss: 0.206456, cmid loss: 0.592523
Epoch (7), Batch(600/2189), loss: 0.788217, imid loss: 0.202799, cmid loss: 0.585419
Epoch (7), Batch(800/2189), loss: 0.788583, imid loss: 0.201447, cmid loss: 0.587136
Epoch (7), Batch(1000/2189), loss: 0.774881, imid loss: 0.200045, cmid loss: 0.574835
Epoch (7), Batch(1200/2189), loss: 0.775393, imid loss: 0.200014, cmid loss: 0.575379
Epoch (7), Batch(1400/2189), loss: 0.769232, imid loss: 0.198062, cmid loss: 0.571170
Epoch (7), Batch(1600/2189), loss: 0.770604, imid loss: 0.197943, cmid loss: 0.572661
Epoch (7), Batch(1800/2189), loss: 0.771712, imid loss: 0.197401, cmid loss: 0.574311
Epoch (7), Batch(2000/2189), loss: 0.776231, imid loss: 0.198614, cmid loss: 0.577618
Train 7, loss: 0.773450
Linear Accuracy : 0.8905996758508914
Start training epoch: (8/100)
Epoch (8), Batch(0/2189), loss: 0.438871, imid loss: 0.091702, cmid loss: 0.347170
Epoch (8), Batch(200/2189), loss: 0.740880, imid loss: 0.193906, cmid loss: 0.546974
Epoch (8), Batch(400/2189), loss: 0.736390, imid loss: 0.189172, cmid loss: 0.547218
Epoch (8), Batch(600/2189), loss: 0.733672, imid loss: 0.187401, cmid loss: 0.546272
Epoch (8), Batch(800/2189), loss: 0.727089, imid loss: 0.185819, cmid loss: 0.541270
Epoch (8), Batch(1000/2189), loss: 0.716251, imid loss: 0.181888, cmid loss: 0.534362
Epoch (8), Batch(1200/2189), loss: 0.718884, imid loss: 0.183920, cmid loss: 0.534964
Epoch (8), Batch(1400/2189), loss: 0.723007, imid loss: 0.184702, cmid loss: 0.538305
Epoch (8), Batch(1600/2189), loss: 0.715025, imid loss: 0.182629, cmid loss: 0.532395
Epoch (8), Batch(1800/2189), loss: 0.713167, imid loss: 0.183069, cmid loss: 0.530098
Epoch (8), Batch(2000/2189), loss: 0.712461, imid loss: 0.183241, cmid loss: 0.529221
Train 8, loss: 0.715364
Linear Accuracy : 0.8881685575364667
Start training epoch: (9/100)
Epoch (9), Batch(0/2189), loss: 0.875544, imid loss: 0.227571, cmid loss: 0.647973
Epoch (9), Batch(200/2189), loss: 0.672352, imid loss: 0.169501, cmid loss: 0.502851
Epoch (9), Batch(400/2189), loss: 0.662104, imid loss: 0.164597, cmid loss: 0.497507
Epoch (9), Batch(600/2189), loss: 0.663720, imid loss: 0.165996, cmid loss: 0.497724
Epoch (9), Batch(800/2189), loss: 0.665034, imid loss: 0.169000, cmid loss: 0.496034
Epoch (9), Batch(1000/2189), loss: 0.661591, imid loss: 0.168905, cmid loss: 0.492685
Epoch (9), Batch(1200/2189), loss: 0.657118, imid loss: 0.168236, cmid loss: 0.488882
Epoch (9), Batch(1400/2189), loss: 0.662083, imid loss: 0.168731, cmid loss: 0.493352
Epoch (9), Batch(1600/2189), loss: 0.656468, imid loss: 0.167372, cmid loss: 0.489096
Epoch (9), Batch(1800/2189), loss: 0.657488, imid loss: 0.167410, cmid loss: 0.490079
Epoch (9), Batch(2000/2189), loss: 0.657006, imid loss: 0.167410, cmid loss: 0.489596
Train 9, loss: 0.656615
Linear Accuracy : 0.8954619124797407
Start training epoch: (10/100)
Epoch (10), Batch(0/2189), loss: 0.652417, imid loss: 0.154075, cmid loss: 0.498342
Epoch (10), Batch(200/2189), loss: 0.605783, imid loss: 0.163914, cmid loss: 0.441869
Epoch (10), Batch(400/2189), loss: 0.624851, imid loss: 0.165237, cmid loss: 0.459614
Epoch (10), Batch(600/2189), loss: 0.632443, imid loss: 0.165907, cmid loss: 0.466536
Epoch (10), Batch(800/2189), loss: 0.638288, imid loss: 0.165875, cmid loss: 0.472413
Epoch (10), Batch(1000/2189), loss: 0.636884, imid loss: 0.165843, cmid loss: 0.471041
Epoch (10), Batch(1200/2189), loss: 0.637061, imid loss: 0.165036, cmid loss: 0.472025
Epoch (10), Batch(1400/2189), loss: 0.634958, imid loss: 0.163862, cmid loss: 0.471096
Epoch (10), Batch(1600/2189), loss: 0.632871, imid loss: 0.162791, cmid loss: 0.470080
Epoch (10), Batch(1800/2189), loss: 0.632017, imid loss: 0.163321, cmid loss: 0.468696
Epoch (10), Batch(2000/2189), loss: 0.629567, imid loss: 0.162006, cmid loss: 0.467560
Train 10, loss: 0.627014
Linear Accuracy : 0.896677471636953
==> Saving...
Start training epoch: (11/100)
Epoch (11), Batch(0/2189), loss: 0.740598, imid loss: 0.136300, cmid loss: 0.604298
Epoch (11), Batch(200/2189), loss: 0.585099, imid loss: 0.139903, cmid loss: 0.445196
Epoch (11), Batch(400/2189), loss: 0.585001, imid loss: 0.142539, cmid loss: 0.442462
Epoch (11), Batch(600/2189), loss: 0.586153, imid loss: 0.148693, cmid loss: 0.437459
Epoch (11), Batch(800/2189), loss: 0.594847, imid loss: 0.150082, cmid loss: 0.444765
Epoch (11), Batch(1000/2189), loss: 0.586692, imid loss: 0.148258, cmid loss: 0.438434
Epoch (11), Batch(1200/2189), loss: 0.588088, imid loss: 0.149558, cmid loss: 0.438530
Epoch (11), Batch(1400/2189), loss: 0.586331, imid loss: 0.149027, cmid loss: 0.437304
Epoch (11), Batch(1600/2189), loss: 0.582193, imid loss: 0.147989, cmid loss: 0.434203
Epoch (11), Batch(1800/2189), loss: 0.579821, imid loss: 0.147214, cmid loss: 0.432608
Epoch (11), Batch(2000/2189), loss: 0.579347, imid loss: 0.147706, cmid loss: 0.431641
Train 11, loss: 0.577019
Linear Accuracy : 0.896677471636953
Start training epoch: (12/100)
Epoch (12), Batch(0/2189), loss: 0.509493, imid loss: 0.135373, cmid loss: 0.374120
Epoch (12), Batch(200/2189), loss: 0.553781, imid loss: 0.140163, cmid loss: 0.413618
Epoch (12), Batch(400/2189), loss: 0.557115, imid loss: 0.145978, cmid loss: 0.411137
Epoch (12), Batch(600/2189), loss: 0.554272, imid loss: 0.141943, cmid loss: 0.412330
Epoch (12), Batch(800/2189), loss: 0.562985, imid loss: 0.142136, cmid loss: 0.420849
Epoch (12), Batch(1000/2189), loss: 0.559695, imid loss: 0.139551, cmid loss: 0.420143
Epoch (12), Batch(1200/2189), loss: 0.559532, imid loss: 0.139803, cmid loss: 0.419729
Epoch (12), Batch(1400/2189), loss: 0.558305, imid loss: 0.138981, cmid loss: 0.419324
Epoch (12), Batch(1600/2189), loss: 0.554922, imid loss: 0.139572, cmid loss: 0.415350
Epoch (12), Batch(1800/2189), loss: 0.553288, imid loss: 0.139910, cmid loss: 0.413378
Epoch (12), Batch(2000/2189), loss: 0.552163, imid loss: 0.140534, cmid loss: 0.411629
Train 12, loss: 0.550545
Linear Accuracy : 0.893030794165316
Start training epoch: (13/100)
Epoch (13), Batch(0/2189), loss: 0.859425, imid loss: 0.304719, cmid loss: 0.554706
Epoch (13), Batch(200/2189), loss: 0.525939, imid loss: 0.134310, cmid loss: 0.391629
Epoch (13), Batch(400/2189), loss: 0.515051, imid loss: 0.129338, cmid loss: 0.385713
Epoch (13), Batch(600/2189), loss: 0.526892, imid loss: 0.132601, cmid loss: 0.394290
Epoch (13), Batch(800/2189), loss: 0.525184, imid loss: 0.134612, cmid loss: 0.390571
Epoch (13), Batch(1000/2189), loss: 0.524934, imid loss: 0.134070, cmid loss: 0.390864
Epoch (13), Batch(1200/2189), loss: 0.522937, imid loss: 0.133142, cmid loss: 0.389795
Epoch (13), Batch(1400/2189), loss: 0.519491, imid loss: 0.132502, cmid loss: 0.386989
Epoch (13), Batch(1600/2189), loss: 0.518947, imid loss: 0.132848, cmid loss: 0.386099
Epoch (13), Batch(1800/2189), loss: 0.520791, imid loss: 0.133869, cmid loss: 0.386922
Epoch (13), Batch(2000/2189), loss: 0.519545, imid loss: 0.133047, cmid loss: 0.386498
Train 13, loss: 0.519341
Linear Accuracy : 0.8978930307941653
Start training epoch: (14/100)
Epoch (14), Batch(0/2189), loss: 0.428390, imid loss: 0.133178, cmid loss: 0.295212
Epoch (14), Batch(200/2189), loss: 0.484954, imid loss: 0.132065, cmid loss: 0.352889
Epoch (14), Batch(400/2189), loss: 0.487293, imid loss: 0.129333, cmid loss: 0.357960
Epoch (14), Batch(600/2189), loss: 0.499925, imid loss: 0.132050, cmid loss: 0.367875
Epoch (14), Batch(800/2189), loss: 0.496609, imid loss: 0.131040, cmid loss: 0.365569
Epoch (14), Batch(1000/2189), loss: 0.493131, imid loss: 0.129206, cmid loss: 0.363925
Epoch (14), Batch(1200/2189), loss: 0.492919, imid loss: 0.129019, cmid loss: 0.363900
Epoch (14), Batch(1400/2189), loss: 0.495536, imid loss: 0.128784, cmid loss: 0.366753
Epoch (14), Batch(1600/2189), loss: 0.495703, imid loss: 0.128996, cmid loss: 0.366707
Epoch (14), Batch(1800/2189), loss: 0.495521, imid loss: 0.128412, cmid loss: 0.367109
Epoch (14), Batch(2000/2189), loss: 0.493890, imid loss: 0.128073, cmid loss: 0.365817
Train 14, loss: 0.493308
Linear Accuracy : 0.8910048622366289
Start training epoch: (15/100)
Epoch (15), Batch(0/2189), loss: 0.520582, imid loss: 0.098445, cmid loss: 0.422137
Epoch (15), Batch(200/2189), loss: 0.459433, imid loss: 0.112565, cmid loss: 0.346867
Epoch (15), Batch(400/2189), loss: 0.458533, imid loss: 0.114561, cmid loss: 0.343972
Epoch (15), Batch(600/2189), loss: 0.460539, imid loss: 0.116081, cmid loss: 0.344458
Epoch (15), Batch(800/2189), loss: 0.463549, imid loss: 0.117322, cmid loss: 0.346227
Epoch (15), Batch(1000/2189), loss: 0.466401, imid loss: 0.116998, cmid loss: 0.349403
Epoch (15), Batch(1200/2189), loss: 0.470624, imid loss: 0.117817, cmid loss: 0.352807
Epoch (15), Batch(1400/2189), loss: 0.467124, imid loss: 0.116701, cmid loss: 0.350424
Epoch (15), Batch(1600/2189), loss: 0.466224, imid loss: 0.116369, cmid loss: 0.349855
Epoch (15), Batch(1800/2189), loss: 0.467390, imid loss: 0.116703, cmid loss: 0.350687
Epoch (15), Batch(2000/2189), loss: 0.466517, imid loss: 0.116673, cmid loss: 0.349844
Train 15, loss: 0.467428
Linear Accuracy : 0.8942463533225283
==> Saving...
Start training epoch: (16/100)
Epoch (16), Batch(0/2189), loss: 0.111710, imid loss: 0.028230, cmid loss: 0.083480
Epoch (16), Batch(200/2189), loss: 0.461633, imid loss: 0.124133, cmid loss: 0.337501
Epoch (16), Batch(400/2189), loss: 0.466879, imid loss: 0.122514, cmid loss: 0.344365
Epoch (16), Batch(600/2189), loss: 0.464432, imid loss: 0.120326, cmid loss: 0.344106
Epoch (16), Batch(800/2189), loss: 0.457117, imid loss: 0.117576, cmid loss: 0.339541
Epoch (16), Batch(1000/2189), loss: 0.452752, imid loss: 0.116306, cmid loss: 0.336446
Epoch (16), Batch(1200/2189), loss: 0.454719, imid loss: 0.118592, cmid loss: 0.336127
Epoch (16), Batch(1400/2189), loss: 0.454600, imid loss: 0.116734, cmid loss: 0.337867
Epoch (16), Batch(1600/2189), loss: 0.451981, imid loss: 0.116055, cmid loss: 0.335926
Epoch (16), Batch(1800/2189), loss: 0.450465, imid loss: 0.115127, cmid loss: 0.335338
Epoch (16), Batch(2000/2189), loss: 0.451054, imid loss: 0.114914, cmid loss: 0.336140
Train 16, loss: 0.450850
Linear Accuracy : 0.8946515397082658
Start training epoch: (17/100)
Epoch (17), Batch(0/2189), loss: 0.269374, imid loss: 0.076574, cmid loss: 0.192800
Epoch (17), Batch(200/2189), loss: 0.421680, imid loss: 0.107959, cmid loss: 0.313721
Epoch (17), Batch(400/2189), loss: 0.441624, imid loss: 0.113304, cmid loss: 0.328320
Epoch (17), Batch(600/2189), loss: 0.436262, imid loss: 0.111819, cmid loss: 0.324443
Epoch (17), Batch(800/2189), loss: 0.435726, imid loss: 0.111587, cmid loss: 0.324139
Epoch (17), Batch(1000/2189), loss: 0.436751, imid loss: 0.111545, cmid loss: 0.325205
Epoch (17), Batch(1200/2189), loss: 0.437471, imid loss: 0.112030, cmid loss: 0.325441
Epoch (17), Batch(1400/2189), loss: 0.440644, imid loss: 0.112426, cmid loss: 0.328218
Epoch (17), Batch(1600/2189), loss: 0.441235, imid loss: 0.112890, cmid loss: 0.328345
Epoch (17), Batch(1800/2189), loss: 0.438635, imid loss: 0.111988, cmid loss: 0.326647
Epoch (17), Batch(2000/2189), loss: 0.436876, imid loss: 0.111456, cmid loss: 0.325420
Train 17, loss: 0.435371
Linear Accuracy : 0.8999189627228525
Start training epoch: (18/100)
Epoch (18), Batch(0/2189), loss: 0.707779, imid loss: 0.190481, cmid loss: 0.517299
Epoch (18), Batch(200/2189), loss: 0.440804, imid loss: 0.113996, cmid loss: 0.326808
Epoch (18), Batch(400/2189), loss: 0.433675, imid loss: 0.109796, cmid loss: 0.323879
Epoch (18), Batch(600/2189), loss: 0.429614, imid loss: 0.109026, cmid loss: 0.320588
Epoch (18), Batch(800/2189), loss: 0.432267, imid loss: 0.108558, cmid loss: 0.323709
Epoch (18), Batch(1000/2189), loss: 0.428751, imid loss: 0.107253, cmid loss: 0.321498
Epoch (18), Batch(1200/2189), loss: 0.428955, imid loss: 0.108052, cmid loss: 0.320902
Epoch (18), Batch(1400/2189), loss: 0.429307, imid loss: 0.108650, cmid loss: 0.320657
Epoch (18), Batch(1600/2189), loss: 0.429777, imid loss: 0.109357, cmid loss: 0.320421
Epoch (18), Batch(1800/2189), loss: 0.430319, imid loss: 0.109883, cmid loss: 0.320436
Epoch (18), Batch(2000/2189), loss: 0.426419, imid loss: 0.108499, cmid loss: 0.317920
Train 18, loss: 0.424279
Linear Accuracy : 0.890194489465154
Start training epoch: (19/100)
Epoch (19), Batch(0/2189), loss: 0.271154, imid loss: 0.068446, cmid loss: 0.202709
Epoch (19), Batch(200/2189), loss: 0.408998, imid loss: 0.106592, cmid loss: 0.302406
Epoch (19), Batch(400/2189), loss: 0.405858, imid loss: 0.104665, cmid loss: 0.301193
Epoch (19), Batch(600/2189), loss: 0.388666, imid loss: 0.100852, cmid loss: 0.287814
Epoch (19), Batch(800/2189), loss: 0.389876, imid loss: 0.101126, cmid loss: 0.288750
Epoch (19), Batch(1000/2189), loss: 0.391898, imid loss: 0.101058, cmid loss: 0.290840
Epoch (19), Batch(1200/2189), loss: 0.396084, imid loss: 0.102528, cmid loss: 0.293557
Epoch (19), Batch(1400/2189), loss: 0.395823, imid loss: 0.102469, cmid loss: 0.293354
Epoch (19), Batch(1600/2189), loss: 0.396857, imid loss: 0.102618, cmid loss: 0.294239
Epoch (19), Batch(1800/2189), loss: 0.395173, imid loss: 0.102073, cmid loss: 0.293099
Epoch (19), Batch(2000/2189), loss: 0.396181, imid loss: 0.102388, cmid loss: 0.293794
Train 19, loss: 0.396999
Linear Accuracy : 0.8942463533225283
Start training epoch: (20/100)
Epoch (20), Batch(0/2189), loss: 0.308345, imid loss: 0.062380, cmid loss: 0.245965
Epoch (20), Batch(200/2189), loss: 0.385590, imid loss: 0.093098, cmid loss: 0.292491
Epoch (20), Batch(400/2189), loss: 0.386743, imid loss: 0.095386, cmid loss: 0.291357
Epoch (20), Batch(600/2189), loss: 0.387612, imid loss: 0.098386, cmid loss: 0.289225
Epoch (20), Batch(800/2189), loss: 0.385229, imid loss: 0.098666, cmid loss: 0.286563
Epoch (20), Batch(1000/2189), loss: 0.385661, imid loss: 0.098590, cmid loss: 0.287071
Epoch (20), Batch(1200/2189), loss: 0.381129, imid loss: 0.097743, cmid loss: 0.283385
Epoch (20), Batch(1400/2189), loss: 0.382143, imid loss: 0.098186, cmid loss: 0.283957
Epoch (20), Batch(1600/2189), loss: 0.380698, imid loss: 0.097634, cmid loss: 0.283065
Epoch (20), Batch(1800/2189), loss: 0.382806, imid loss: 0.098222, cmid loss: 0.284584
Epoch (20), Batch(2000/2189), loss: 0.383570, imid loss: 0.097779, cmid loss: 0.285790
Train 20, loss: 0.384648
Linear Accuracy : 0.8897893030794165
==> Saving...
Start training epoch: (21/100)
Epoch (21), Batch(0/2189), loss: 0.566943, imid loss: 0.159849, cmid loss: 0.407094
Epoch (21), Batch(200/2189), loss: 0.387956, imid loss: 0.101697, cmid loss: 0.286259
Epoch (21), Batch(400/2189), loss: 0.411028, imid loss: 0.106827, cmid loss: 0.304202
Epoch (21), Batch(600/2189), loss: 0.398210, imid loss: 0.103269, cmid loss: 0.294941
Epoch (21), Batch(800/2189), loss: 0.394020, imid loss: 0.102549, cmid loss: 0.291470
Epoch (21), Batch(1000/2189), loss: 0.390048, imid loss: 0.100755, cmid loss: 0.289293
Epoch (21), Batch(1200/2189), loss: 0.387081, imid loss: 0.100475, cmid loss: 0.286607
Epoch (21), Batch(1400/2189), loss: 0.386957, imid loss: 0.099214, cmid loss: 0.287742
Epoch (21), Batch(1600/2189), loss: 0.384229, imid loss: 0.098287, cmid loss: 0.285942
Epoch (21), Batch(1800/2189), loss: 0.381666, imid loss: 0.097814, cmid loss: 0.283852
Epoch (21), Batch(2000/2189), loss: 0.378789, imid loss: 0.097260, cmid loss: 0.281529
Train 21, loss: 0.379218
Linear Accuracy : 0.8910048622366289
Start training epoch: (22/100)
Epoch (22), Batch(0/2189), loss: 0.307622, imid loss: 0.121402, cmid loss: 0.186219
Epoch (22), Batch(200/2189), loss: 0.343445, imid loss: 0.083299, cmid loss: 0.260146
Epoch (22), Batch(400/2189), loss: 0.364286, imid loss: 0.087111, cmid loss: 0.277175
Epoch (22), Batch(600/2189), loss: 0.364013, imid loss: 0.089011, cmid loss: 0.275002
Epoch (22), Batch(800/2189), loss: 0.364292, imid loss: 0.090489, cmid loss: 0.273803
Epoch (22), Batch(1000/2189), loss: 0.361224, imid loss: 0.092017, cmid loss: 0.269207
Epoch (22), Batch(1200/2189), loss: 0.361243, imid loss: 0.092260, cmid loss: 0.268983
Epoch (22), Batch(1400/2189), loss: 0.363148, imid loss: 0.092804, cmid loss: 0.270344
Epoch (22), Batch(1600/2189), loss: 0.366713, imid loss: 0.093902, cmid loss: 0.272812
Epoch (22), Batch(1800/2189), loss: 0.365548, imid loss: 0.093903, cmid loss: 0.271644
Epoch (22), Batch(2000/2189), loss: 0.365133, imid loss: 0.093902, cmid loss: 0.271231
Train 22, loss: 0.364025
Linear Accuracy : 0.8950567260940032
Start training epoch: (23/100)
Epoch (23), Batch(0/2189), loss: 0.378085, imid loss: 0.177946, cmid loss: 0.200139
Epoch (23), Batch(200/2189), loss: 0.377026, imid loss: 0.100689, cmid loss: 0.276337
Epoch (23), Batch(400/2189), loss: 0.365736, imid loss: 0.096792, cmid loss: 0.268944
Epoch (23), Batch(600/2189), loss: 0.346956, imid loss: 0.091126, cmid loss: 0.255829
Epoch (23), Batch(800/2189), loss: 0.352557, imid loss: 0.092437, cmid loss: 0.260120
Epoch (23), Batch(1000/2189), loss: 0.351110, imid loss: 0.092380, cmid loss: 0.258730
Epoch (23), Batch(1200/2189), loss: 0.351842, imid loss: 0.091703, cmid loss: 0.260140
Epoch (23), Batch(1400/2189), loss: 0.349975, imid loss: 0.091441, cmid loss: 0.258534
Epoch (23), Batch(1600/2189), loss: 0.350881, imid loss: 0.091674, cmid loss: 0.259207
Epoch (23), Batch(1800/2189), loss: 0.352836, imid loss: 0.091895, cmid loss: 0.260941
Epoch (23), Batch(2000/2189), loss: 0.351657, imid loss: 0.091352, cmid loss: 0.260304
Train 23, loss: 0.354057
Linear Accuracy : 0.8958670988654781
Start training epoch: (24/100)
Epoch (24), Batch(0/2189), loss: 0.486699, imid loss: 0.083694, cmid loss: 0.403005
Epoch (24), Batch(200/2189), loss: 0.331027, imid loss: 0.091109, cmid loss: 0.239917
Epoch (24), Batch(400/2189), loss: 0.335387, imid loss: 0.091897, cmid loss: 0.243489
Epoch (24), Batch(600/2189), loss: 0.329670, imid loss: 0.088832, cmid loss: 0.240838
Epoch (24), Batch(800/2189), loss: 0.328393, imid loss: 0.087643, cmid loss: 0.240750
Epoch (24), Batch(1000/2189), loss: 0.334213, imid loss: 0.088371, cmid loss: 0.245842
Epoch (24), Batch(1200/2189), loss: 0.334886, imid loss: 0.088499, cmid loss: 0.246387
Epoch (24), Batch(1400/2189), loss: 0.334140, imid loss: 0.088016, cmid loss: 0.246124
Epoch (24), Batch(1600/2189), loss: 0.338881, imid loss: 0.089051, cmid loss: 0.249831
Epoch (24), Batch(1800/2189), loss: 0.338188, imid loss: 0.088728, cmid loss: 0.249460
Epoch (24), Batch(2000/2189), loss: 0.336893, imid loss: 0.088016, cmid loss: 0.248877
Train 24, loss: 0.338318
Linear Accuracy : 0.8978930307941653
Start training epoch: (25/100)
Epoch (25), Batch(0/2189), loss: 0.282862, imid loss: 0.029826, cmid loss: 0.253036
Epoch (25), Batch(200/2189), loss: 0.324837, imid loss: 0.079940, cmid loss: 0.244897
Epoch (25), Batch(400/2189), loss: 0.315774, imid loss: 0.079633, cmid loss: 0.236141
Epoch (25), Batch(600/2189), loss: 0.326595, imid loss: 0.084234, cmid loss: 0.242361
Epoch (25), Batch(800/2189), loss: 0.330560, imid loss: 0.085944, cmid loss: 0.244616
Epoch (25), Batch(1000/2189), loss: 0.331796, imid loss: 0.086751, cmid loss: 0.245045
Epoch (25), Batch(1200/2189), loss: 0.331625, imid loss: 0.086704, cmid loss: 0.244920
Epoch (25), Batch(1400/2189), loss: 0.332996, imid loss: 0.087217, cmid loss: 0.245779
Epoch (25), Batch(1600/2189), loss: 0.331983, imid loss: 0.086614, cmid loss: 0.245369
Epoch (25), Batch(1800/2189), loss: 0.332314, imid loss: 0.086392, cmid loss: 0.245922
Epoch (25), Batch(2000/2189), loss: 0.333842, imid loss: 0.086784, cmid loss: 0.247058
Train 25, loss: 0.334708
Linear Accuracy : 0.8991085899513777
==> Saving...
Start training epoch: (26/100)
Epoch (26), Batch(0/2189), loss: 0.180518, imid loss: 0.029058, cmid loss: 0.151460
Epoch (26), Batch(200/2189), loss: 0.327819, imid loss: 0.083690, cmid loss: 0.244129
Epoch (26), Batch(400/2189), loss: 0.323416, imid loss: 0.081004, cmid loss: 0.242413
Epoch (26), Batch(600/2189), loss: 0.317475, imid loss: 0.081049, cmid loss: 0.236426
Epoch (26), Batch(800/2189), loss: 0.320511, imid loss: 0.082202, cmid loss: 0.238309
Epoch (26), Batch(1000/2189), loss: 0.319713, imid loss: 0.081564, cmid loss: 0.238149
Epoch (26), Batch(1200/2189), loss: 0.322605, imid loss: 0.082029, cmid loss: 0.240577
Epoch (26), Batch(1400/2189), loss: 0.318933, imid loss: 0.081275, cmid loss: 0.237658
Epoch (26), Batch(1600/2189), loss: 0.319545, imid loss: 0.081869, cmid loss: 0.237676
Epoch (26), Batch(1800/2189), loss: 0.319352, imid loss: 0.082174, cmid loss: 0.237178
Epoch (26), Batch(2000/2189), loss: 0.317532, imid loss: 0.081792, cmid loss: 0.235740
Train 26, loss: 0.316421
Linear Accuracy : 0.8938411669367909
Start training epoch: (27/100)
Epoch (27), Batch(0/2189), loss: 0.333168, imid loss: 0.083755, cmid loss: 0.249413
Epoch (27), Batch(200/2189), loss: 0.325500, imid loss: 0.085064, cmid loss: 0.240436
Epoch (27), Batch(400/2189), loss: 0.303054, imid loss: 0.079532, cmid loss: 0.223523
Epoch (27), Batch(600/2189), loss: 0.302858, imid loss: 0.080593, cmid loss: 0.222264
Epoch (27), Batch(800/2189), loss: 0.309925, imid loss: 0.082814, cmid loss: 0.227111
Epoch (27), Batch(1000/2189), loss: 0.310094, imid loss: 0.082596, cmid loss: 0.227497
Epoch (27), Batch(1200/2189), loss: 0.306328, imid loss: 0.081404, cmid loss: 0.224924
Epoch (27), Batch(1400/2189), loss: 0.307711, imid loss: 0.081154, cmid loss: 0.226557
Epoch (27), Batch(1600/2189), loss: 0.309000, imid loss: 0.081287, cmid loss: 0.227713
Epoch (27), Batch(1800/2189), loss: 0.308912, imid loss: 0.080685, cmid loss: 0.228227
Epoch (27), Batch(2000/2189), loss: 0.309273, imid loss: 0.080735, cmid loss: 0.228538
Train 27, loss: 0.311031
Linear Accuracy : 0.8950567260940032
Start training epoch: (28/100)
Epoch (28), Batch(0/2189), loss: 0.437545, imid loss: 0.046026, cmid loss: 0.391519
Epoch (28), Batch(200/2189), loss: 0.320173, imid loss: 0.082740, cmid loss: 0.237433
Epoch (28), Batch(400/2189), loss: 0.309358, imid loss: 0.080690, cmid loss: 0.228668
Epoch (28), Batch(600/2189), loss: 0.311600, imid loss: 0.081243, cmid loss: 0.230357
Epoch (28), Batch(800/2189), loss: 0.305811, imid loss: 0.079538, cmid loss: 0.226273
Epoch (28), Batch(1000/2189), loss: 0.305028, imid loss: 0.079251, cmid loss: 0.225778
Epoch (28), Batch(1200/2189), loss: 0.306330, imid loss: 0.079291, cmid loss: 0.227039
Epoch (28), Batch(1400/2189), loss: 0.307315, imid loss: 0.079175, cmid loss: 0.228140
Epoch (28), Batch(1600/2189), loss: 0.305481, imid loss: 0.078648, cmid loss: 0.226833
Epoch (28), Batch(1800/2189), loss: 0.304349, imid loss: 0.077928, cmid loss: 0.226421
Epoch (28), Batch(2000/2189), loss: 0.304478, imid loss: 0.078076, cmid loss: 0.226403
Train 28, loss: 0.303563
Linear Accuracy : 0.8962722852512156
Start training epoch: (29/100)
Epoch (29), Batch(0/2189), loss: 0.248935, imid loss: 0.088987, cmid loss: 0.159947
Epoch (29), Batch(200/2189), loss: 0.285964, imid loss: 0.079422, cmid loss: 0.206543
Epoch (29), Batch(400/2189), loss: 0.286200, imid loss: 0.077231, cmid loss: 0.208969
Epoch (29), Batch(600/2189), loss: 0.284525, imid loss: 0.074472, cmid loss: 0.210053
Epoch (29), Batch(800/2189), loss: 0.284864, imid loss: 0.075110, cmid loss: 0.209754
Epoch (29), Batch(1000/2189), loss: 0.285906, imid loss: 0.074050, cmid loss: 0.211856
Epoch (29), Batch(1200/2189), loss: 0.285546, imid loss: 0.073701, cmid loss: 0.211845
Epoch (29), Batch(1400/2189), loss: 0.287643, imid loss: 0.074848, cmid loss: 0.212796
Epoch (29), Batch(1600/2189), loss: 0.287521, imid loss: 0.074928, cmid loss: 0.212593
Epoch (29), Batch(1800/2189), loss: 0.288956, imid loss: 0.075298, cmid loss: 0.213659
Epoch (29), Batch(2000/2189), loss: 0.290467, imid loss: 0.075337, cmid loss: 0.215130
Train 29, loss: 0.291786
Linear Accuracy : 0.8926256077795786
Start training epoch: (30/100)
Epoch (30), Batch(0/2189), loss: 0.109489, imid loss: 0.016968, cmid loss: 0.092521
Epoch (30), Batch(200/2189), loss: 0.270275, imid loss: 0.070360, cmid loss: 0.199915
Epoch (30), Batch(400/2189), loss: 0.287345, imid loss: 0.076707, cmid loss: 0.210638
Epoch (30), Batch(600/2189), loss: 0.286230, imid loss: 0.077019, cmid loss: 0.209211
Epoch (30), Batch(800/2189), loss: 0.285814, imid loss: 0.075679, cmid loss: 0.210135
Epoch (30), Batch(1000/2189), loss: 0.286279, imid loss: 0.075340, cmid loss: 0.210940
Epoch (30), Batch(1200/2189), loss: 0.286228, imid loss: 0.074597, cmid loss: 0.211631
Epoch (30), Batch(1400/2189), loss: 0.286003, imid loss: 0.074289, cmid loss: 0.211714
Epoch (30), Batch(1600/2189), loss: 0.287283, imid loss: 0.074511, cmid loss: 0.212772
Epoch (30), Batch(1800/2189), loss: 0.288253, imid loss: 0.074597, cmid loss: 0.213656
Epoch (30), Batch(2000/2189), loss: 0.288753, imid loss: 0.074638, cmid loss: 0.214115
Train 30, loss: 0.288655
Linear Accuracy : 0.9011345218800648
==> Saving...
Start training epoch: (31/100)
Epoch (31), Batch(0/2189), loss: 0.343583, imid loss: 0.077712, cmid loss: 0.265871
Epoch (31), Batch(200/2189), loss: 0.270891, imid loss: 0.069379, cmid loss: 0.201513
Epoch (31), Batch(400/2189), loss: 0.276995, imid loss: 0.069386, cmid loss: 0.207609
Epoch (31), Batch(600/2189), loss: 0.279943, imid loss: 0.072132, cmid loss: 0.207811
Epoch (31), Batch(800/2189), loss: 0.280811, imid loss: 0.072819, cmid loss: 0.207992
Epoch (31), Batch(1000/2189), loss: 0.277876, imid loss: 0.071605, cmid loss: 0.206272
Epoch (31), Batch(1200/2189), loss: 0.279466, imid loss: 0.071955, cmid loss: 0.207511
Epoch (31), Batch(1400/2189), loss: 0.276846, imid loss: 0.071327, cmid loss: 0.205519
Epoch (31), Batch(1600/2189), loss: 0.274833, imid loss: 0.071124, cmid loss: 0.203709
Epoch (31), Batch(1800/2189), loss: 0.276058, imid loss: 0.071111, cmid loss: 0.204947
Epoch (31), Batch(2000/2189), loss: 0.276734, imid loss: 0.071646, cmid loss: 0.205088
Train 31, loss: 0.276564
Linear Accuracy : 0.8922204213938412
Start training epoch: (32/100)
Epoch (32), Batch(0/2189), loss: 0.123136, imid loss: 0.028495, cmid loss: 0.094640
Epoch (32), Batch(200/2189), loss: 0.251892, imid loss: 0.064015, cmid loss: 0.187876
Epoch (32), Batch(400/2189), loss: 0.264336, imid loss: 0.070189, cmid loss: 0.194147
Epoch (32), Batch(600/2189), loss: 0.265631, imid loss: 0.070995, cmid loss: 0.194636
Epoch (32), Batch(800/2189), loss: 0.263937, imid loss: 0.070650, cmid loss: 0.193287
Epoch (32), Batch(1000/2189), loss: 0.260714, imid loss: 0.069656, cmid loss: 0.191058
Epoch (32), Batch(1200/2189), loss: 0.261543, imid loss: 0.068974, cmid loss: 0.192569
Epoch (32), Batch(1400/2189), loss: 0.264841, imid loss: 0.069788, cmid loss: 0.195054
Epoch (32), Batch(1600/2189), loss: 0.264162, imid loss: 0.070057, cmid loss: 0.194105
Epoch (32), Batch(1800/2189), loss: 0.265021, imid loss: 0.069694, cmid loss: 0.195327
Epoch (32), Batch(2000/2189), loss: 0.264570, imid loss: 0.069339, cmid loss: 0.195231
Train 32, loss: 0.265555
Linear Accuracy : 0.8954619124797407
Start training epoch: (33/100)
Epoch (33), Batch(0/2189), loss: 0.326575, imid loss: 0.103647, cmid loss: 0.222928
Epoch (33), Batch(200/2189), loss: 0.266362, imid loss: 0.072957, cmid loss: 0.193405
Epoch (33), Batch(400/2189), loss: 0.265764, imid loss: 0.074489, cmid loss: 0.191275
Epoch (33), Batch(600/2189), loss: 0.268941, imid loss: 0.073109, cmid loss: 0.195832
Epoch (33), Batch(800/2189), loss: 0.271113, imid loss: 0.071780, cmid loss: 0.199333
Epoch (33), Batch(1000/2189), loss: 0.271487, imid loss: 0.072105, cmid loss: 0.199382
Epoch (33), Batch(1200/2189), loss: 0.269727, imid loss: 0.071497, cmid loss: 0.198230
Epoch (33), Batch(1400/2189), loss: 0.266123, imid loss: 0.070455, cmid loss: 0.195669
Epoch (33), Batch(1600/2189), loss: 0.266189, imid loss: 0.070656, cmid loss: 0.195534
Epoch (33), Batch(1800/2189), loss: 0.267843, imid loss: 0.071243, cmid loss: 0.196600
Epoch (33), Batch(2000/2189), loss: 0.266821, imid loss: 0.071291, cmid loss: 0.195530
Train 33, loss: 0.266782
Linear Accuracy : 0.8962722852512156
Start training epoch: (34/100)
Epoch (34), Batch(0/2189), loss: 0.189115, imid loss: 0.048436, cmid loss: 0.140679
Epoch (34), Batch(200/2189), loss: 0.268300, imid loss: 0.071483, cmid loss: 0.196817
Epoch (34), Batch(400/2189), loss: 0.271928, imid loss: 0.073436, cmid loss: 0.198492
Epoch (34), Batch(600/2189), loss: 0.268096, imid loss: 0.072019, cmid loss: 0.196077
Epoch (34), Batch(800/2189), loss: 0.268699, imid loss: 0.071036, cmid loss: 0.197662
Epoch (34), Batch(1000/2189), loss: 0.268994, imid loss: 0.070923, cmid loss: 0.198071
Epoch (34), Batch(1200/2189), loss: 0.267953, imid loss: 0.069490, cmid loss: 0.198463
Epoch (34), Batch(1400/2189), loss: 0.267312, imid loss: 0.069250, cmid loss: 0.198062
Epoch (34), Batch(1600/2189), loss: 0.268100, imid loss: 0.070108, cmid loss: 0.197992
Epoch (34), Batch(1800/2189), loss: 0.264243, imid loss: 0.068852, cmid loss: 0.195392
Epoch (34), Batch(2000/2189), loss: 0.262031, imid loss: 0.067956, cmid loss: 0.194075
Train 34, loss: 0.261821
Linear Accuracy : 0.8914100486223663
Start training epoch: (35/100)
Epoch (35), Batch(0/2189), loss: 0.176623, imid loss: 0.022958, cmid loss: 0.153665
Epoch (35), Batch(200/2189), loss: 0.268363, imid loss: 0.071324, cmid loss: 0.197038
Epoch (35), Batch(400/2189), loss: 0.269689, imid loss: 0.070746, cmid loss: 0.198942
Epoch (35), Batch(600/2189), loss: 0.261891, imid loss: 0.066781, cmid loss: 0.195110
Epoch (35), Batch(800/2189), loss: 0.257898, imid loss: 0.065847, cmid loss: 0.192050
Epoch (35), Batch(1000/2189), loss: 0.259612, imid loss: 0.066722, cmid loss: 0.192890
Epoch (35), Batch(1200/2189), loss: 0.260978, imid loss: 0.067083, cmid loss: 0.193895
Epoch (35), Batch(1400/2189), loss: 0.260800, imid loss: 0.067338, cmid loss: 0.193461
Epoch (35), Batch(1600/2189), loss: 0.260536, imid loss: 0.067240, cmid loss: 0.193296
Epoch (35), Batch(1800/2189), loss: 0.259696, imid loss: 0.067562, cmid loss: 0.192133
Epoch (35), Batch(2000/2189), loss: 0.258740, imid loss: 0.067639, cmid loss: 0.191101
Train 35, loss: 0.257410
Linear Accuracy : 0.8934359805510534
==> Saving...
Start training epoch: (36/100)
Epoch (36), Batch(0/2189), loss: 0.141959, imid loss: 0.027840, cmid loss: 0.114119
Epoch (36), Batch(200/2189), loss: 0.271377, imid loss: 0.071281, cmid loss: 0.200096
Epoch (36), Batch(400/2189), loss: 0.264896, imid loss: 0.067935, cmid loss: 0.196961
Epoch (36), Batch(600/2189), loss: 0.263272, imid loss: 0.067421, cmid loss: 0.195851
Epoch (36), Batch(800/2189), loss: 0.253916, imid loss: 0.066149, cmid loss: 0.187766
Epoch (36), Batch(1000/2189), loss: 0.253067, imid loss: 0.066055, cmid loss: 0.187013
Epoch (36), Batch(1200/2189), loss: 0.248954, imid loss: 0.064271, cmid loss: 0.184683
Epoch (36), Batch(1400/2189), loss: 0.247905, imid loss: 0.063970, cmid loss: 0.183935
Epoch (36), Batch(1600/2189), loss: 0.248515, imid loss: 0.063941, cmid loss: 0.184574
Epoch (36), Batch(1800/2189), loss: 0.248752, imid loss: 0.063813, cmid loss: 0.184939
Epoch (36), Batch(2000/2189), loss: 0.249899, imid loss: 0.064477, cmid loss: 0.185422
Train 36, loss: 0.249527
Linear Accuracy : 0.8982982171799028
Start training epoch: (37/100)
Epoch (37), Batch(0/2189), loss: 0.168322, imid loss: 0.024323, cmid loss: 0.143999
Epoch (37), Batch(200/2189), loss: 0.263349, imid loss: 0.070155, cmid loss: 0.193194
Epoch (37), Batch(400/2189), loss: 0.252092, imid loss: 0.066632, cmid loss: 0.185460
Epoch (37), Batch(600/2189), loss: 0.247241, imid loss: 0.065213, cmid loss: 0.182028
Epoch (37), Batch(800/2189), loss: 0.243677, imid loss: 0.063518, cmid loss: 0.180158
Epoch (37), Batch(1000/2189), loss: 0.245585, imid loss: 0.064815, cmid loss: 0.180770
Epoch (37), Batch(1200/2189), loss: 0.247256, imid loss: 0.066101, cmid loss: 0.181155
Epoch (37), Batch(1400/2189), loss: 0.248477, imid loss: 0.066395, cmid loss: 0.182083
Epoch (37), Batch(1600/2189), loss: 0.247901, imid loss: 0.065983, cmid loss: 0.181918
Epoch (37), Batch(1800/2189), loss: 0.247598, imid loss: 0.065616, cmid loss: 0.181982
Epoch (37), Batch(2000/2189), loss: 0.246108, imid loss: 0.064964, cmid loss: 0.181143
Train 37, loss: 0.245713
Linear Accuracy : 0.8958670988654781
Start training epoch: (38/100)
Epoch (38), Batch(0/2189), loss: 0.133128, imid loss: 0.022895, cmid loss: 0.110233
Epoch (38), Batch(200/2189), loss: 0.251789, imid loss: 0.064206, cmid loss: 0.187582
Epoch (38), Batch(400/2189), loss: 0.243848, imid loss: 0.062292, cmid loss: 0.181555
Epoch (38), Batch(600/2189), loss: 0.246059, imid loss: 0.063197, cmid loss: 0.182863
Epoch (38), Batch(800/2189), loss: 0.245353, imid loss: 0.063878, cmid loss: 0.181475
Epoch (38), Batch(1000/2189), loss: 0.247633, imid loss: 0.064939, cmid loss: 0.182694
Epoch (38), Batch(1200/2189), loss: 0.246513, imid loss: 0.065025, cmid loss: 0.181487
Epoch (38), Batch(1400/2189), loss: 0.247136, imid loss: 0.064638, cmid loss: 0.182498
Epoch (38), Batch(1600/2189), loss: 0.247075, imid loss: 0.064302, cmid loss: 0.182773
Epoch (38), Batch(1800/2189), loss: 0.244096, imid loss: 0.063170, cmid loss: 0.180925
Epoch (38), Batch(2000/2189), loss: 0.242375, imid loss: 0.062953, cmid loss: 0.179422
Train 38, loss: 0.240816
Linear Accuracy : 0.8987034035656402
Start training epoch: (39/100)
Epoch (39), Batch(0/2189), loss: 0.213235, imid loss: 0.066075, cmid loss: 0.147161
Epoch (39), Batch(200/2189), loss: 0.227898, imid loss: 0.064908, cmid loss: 0.162990
Epoch (39), Batch(400/2189), loss: 0.221363, imid loss: 0.062146, cmid loss: 0.159217
Epoch (39), Batch(600/2189), loss: 0.226873, imid loss: 0.062759, cmid loss: 0.164114
Epoch (39), Batch(800/2189), loss: 0.230297, imid loss: 0.062496, cmid loss: 0.167801
Epoch (39), Batch(1000/2189), loss: 0.229596, imid loss: 0.062108, cmid loss: 0.167488
Epoch (39), Batch(1200/2189), loss: 0.231573, imid loss: 0.062058, cmid loss: 0.169514
Epoch (39), Batch(1400/2189), loss: 0.230596, imid loss: 0.061312, cmid loss: 0.169283
Epoch (39), Batch(1600/2189), loss: 0.229954, imid loss: 0.061410, cmid loss: 0.168545
Epoch (39), Batch(1800/2189), loss: 0.231055, imid loss: 0.061416, cmid loss: 0.169640
Epoch (39), Batch(2000/2189), loss: 0.229502, imid loss: 0.061372, cmid loss: 0.168131
Train 39, loss: 0.229349
Linear Accuracy : 0.9023500810372771
==> Saving Best Model...
Start training epoch: (40/100)
Epoch (40), Batch(0/2189), loss: 0.310495, imid loss: 0.045780, cmid loss: 0.264714
Epoch (40), Batch(200/2189), loss: 0.218943, imid loss: 0.059918, cmid loss: 0.159025
Epoch (40), Batch(400/2189), loss: 0.223533, imid loss: 0.059276, cmid loss: 0.164257
Epoch (40), Batch(600/2189), loss: 0.221837, imid loss: 0.059123, cmid loss: 0.162714
Epoch (40), Batch(800/2189), loss: 0.225461, imid loss: 0.059672, cmid loss: 0.165789
Epoch (40), Batch(1000/2189), loss: 0.228622, imid loss: 0.060854, cmid loss: 0.167768
Epoch (40), Batch(1200/2189), loss: 0.227272, imid loss: 0.060918, cmid loss: 0.166354
Epoch (40), Batch(1400/2189), loss: 0.227655, imid loss: 0.060496, cmid loss: 0.167160
Epoch (40), Batch(1600/2189), loss: 0.226295, imid loss: 0.060522, cmid loss: 0.165772
Epoch (40), Batch(1800/2189), loss: 0.226363, imid loss: 0.059740, cmid loss: 0.166622
Epoch (40), Batch(2000/2189), loss: 0.227253, imid loss: 0.059966, cmid loss: 0.167287
Train 40, loss: 0.226499
Linear Accuracy : 0.9055915721231766
==> Saving Best Model...
==> Saving...
Start training epoch: (41/100)
Epoch (41), Batch(0/2189), loss: 0.172268, imid loss: 0.020525, cmid loss: 0.151744
Epoch (41), Batch(200/2189), loss: 0.236236, imid loss: 0.061189, cmid loss: 0.175047
Epoch (41), Batch(400/2189), loss: 0.225950, imid loss: 0.059473, cmid loss: 0.166477
Epoch (41), Batch(600/2189), loss: 0.223241, imid loss: 0.058723, cmid loss: 0.164518
Epoch (41), Batch(800/2189), loss: 0.223898, imid loss: 0.058869, cmid loss: 0.165029
Epoch (41), Batch(1000/2189), loss: 0.223482, imid loss: 0.058334, cmid loss: 0.165149
Epoch (41), Batch(1200/2189), loss: 0.223578, imid loss: 0.058728, cmid loss: 0.164850
Epoch (41), Batch(1400/2189), loss: 0.221512, imid loss: 0.058319, cmid loss: 0.163193
Epoch (41), Batch(1600/2189), loss: 0.220913, imid loss: 0.058716, cmid loss: 0.162197
Epoch (41), Batch(1800/2189), loss: 0.222619, imid loss: 0.059581, cmid loss: 0.163038
Epoch (41), Batch(2000/2189), loss: 0.222930, imid loss: 0.059650, cmid loss: 0.163280
Train 41, loss: 0.223890
Linear Accuracy : 0.899513776337115
Start training epoch: (42/100)
Epoch (42), Batch(0/2189), loss: 0.202729, imid loss: 0.065492, cmid loss: 0.137236
Epoch (42), Batch(200/2189), loss: 0.223415, imid loss: 0.062652, cmid loss: 0.160763
Epoch (42), Batch(400/2189), loss: 0.220114, imid loss: 0.060870, cmid loss: 0.159244
Epoch (42), Batch(600/2189), loss: 0.215441, imid loss: 0.058600, cmid loss: 0.156841
Epoch (42), Batch(800/2189), loss: 0.219127, imid loss: 0.058684, cmid loss: 0.160443
Epoch (42), Batch(1000/2189), loss: 0.220453, imid loss: 0.058456, cmid loss: 0.161997
Epoch (42), Batch(1200/2189), loss: 0.219314, imid loss: 0.057595, cmid loss: 0.161719
Epoch (42), Batch(1400/2189), loss: 0.218155, imid loss: 0.057151, cmid loss: 0.161004
Epoch (42), Batch(1600/2189), loss: 0.217529, imid loss: 0.057229, cmid loss: 0.160299
Epoch (42), Batch(1800/2189), loss: 0.218481, imid loss: 0.057545, cmid loss: 0.160936
Epoch (42), Batch(2000/2189), loss: 0.218314, imid loss: 0.057786, cmid loss: 0.160528
Train 42, loss: 0.217582
Linear Accuracy : 0.8970826580226904
Start training epoch: (43/100)
Epoch (43), Batch(0/2189), loss: 0.151722, imid loss: 0.025032, cmid loss: 0.126690
Epoch (43), Batch(200/2189), loss: 0.203164, imid loss: 0.055091, cmid loss: 0.148073
Epoch (43), Batch(400/2189), loss: 0.202958, imid loss: 0.055202, cmid loss: 0.147756
Epoch (43), Batch(600/2189), loss: 0.207071, imid loss: 0.054531, cmid loss: 0.152539
Epoch (43), Batch(800/2189), loss: 0.207436, imid loss: 0.054831, cmid loss: 0.152606
Epoch (43), Batch(1000/2189), loss: 0.209123, imid loss: 0.055463, cmid loss: 0.153659
Epoch (43), Batch(1200/2189), loss: 0.212719, imid loss: 0.056120, cmid loss: 0.156599
Epoch (43), Batch(1400/2189), loss: 0.211668, imid loss: 0.055386, cmid loss: 0.156282
Epoch (43), Batch(1600/2189), loss: 0.212785, imid loss: 0.055858, cmid loss: 0.156927
Epoch (43), Batch(1800/2189), loss: 0.212513, imid loss: 0.056054, cmid loss: 0.156459
Epoch (43), Batch(2000/2189), loss: 0.212530, imid loss: 0.055750, cmid loss: 0.156780
Train 43, loss: 0.213369
Linear Accuracy : 0.9039708265802269
Start training epoch: (44/100)
Epoch (44), Batch(0/2189), loss: 0.469393, imid loss: 0.075976, cmid loss: 0.393416
Epoch (44), Batch(200/2189), loss: 0.214607, imid loss: 0.060431, cmid loss: 0.154175
Epoch (44), Batch(400/2189), loss: 0.209986, imid loss: 0.056222, cmid loss: 0.153764
Epoch (44), Batch(600/2189), loss: 0.210838, imid loss: 0.057064, cmid loss: 0.153773
Epoch (44), Batch(800/2189), loss: 0.215196, imid loss: 0.058555, cmid loss: 0.156642
Epoch (44), Batch(1000/2189), loss: 0.215047, imid loss: 0.058327, cmid loss: 0.156720
Epoch (44), Batch(1200/2189), loss: 0.214778, imid loss: 0.058732, cmid loss: 0.156046
Epoch (44), Batch(1400/2189), loss: 0.215190, imid loss: 0.058348, cmid loss: 0.156842
Epoch (44), Batch(1600/2189), loss: 0.214489, imid loss: 0.058804, cmid loss: 0.155685
Epoch (44), Batch(1800/2189), loss: 0.213705, imid loss: 0.058096, cmid loss: 0.155609
Epoch (44), Batch(2000/2189), loss: 0.214352, imid loss: 0.057848, cmid loss: 0.156504
Train 44, loss: 0.213560
Linear Accuracy : 0.9039708265802269
Start training epoch: (45/100)
Epoch (45), Batch(0/2189), loss: 0.175723, imid loss: 0.038556, cmid loss: 0.137167
Epoch (45), Batch(200/2189), loss: 0.225495, imid loss: 0.059641, cmid loss: 0.165853
Epoch (45), Batch(400/2189), loss: 0.211160, imid loss: 0.059042, cmid loss: 0.152119
Epoch (45), Batch(600/2189), loss: 0.208508, imid loss: 0.057761, cmid loss: 0.150747
Epoch (45), Batch(800/2189), loss: 0.208245, imid loss: 0.057190, cmid loss: 0.151055
Epoch (45), Batch(1000/2189), loss: 0.206039, imid loss: 0.056254, cmid loss: 0.149786
Epoch (45), Batch(1200/2189), loss: 0.206695, imid loss: 0.055860, cmid loss: 0.150835
Epoch (45), Batch(1400/2189), loss: 0.204719, imid loss: 0.055099, cmid loss: 0.149621
Epoch (45), Batch(1600/2189), loss: 0.203550, imid loss: 0.054310, cmid loss: 0.149240
Epoch (45), Batch(1800/2189), loss: 0.202438, imid loss: 0.053691, cmid loss: 0.148747
Epoch (45), Batch(2000/2189), loss: 0.204658, imid loss: 0.054261, cmid loss: 0.150398
Train 45, loss: 0.204417
Linear Accuracy : 0.9003241491085899
==> Saving...
Start training epoch: (46/100)
Epoch (46), Batch(0/2189), loss: 0.181553, imid loss: 0.075485, cmid loss: 0.106068
Epoch (46), Batch(200/2189), loss: 0.205008, imid loss: 0.056304, cmid loss: 0.148704
Epoch (46), Batch(400/2189), loss: 0.203107, imid loss: 0.055294, cmid loss: 0.147813
Epoch (46), Batch(600/2189), loss: 0.204472, imid loss: 0.055551, cmid loss: 0.148921
Epoch (46), Batch(800/2189), loss: 0.204383, imid loss: 0.054089, cmid loss: 0.150294
Epoch (46), Batch(1000/2189), loss: 0.204385, imid loss: 0.053537, cmid loss: 0.150848
Epoch (46), Batch(1200/2189), loss: 0.205926, imid loss: 0.054078, cmid loss: 0.151848
Epoch (46), Batch(1400/2189), loss: 0.205349, imid loss: 0.054056, cmid loss: 0.151294
Epoch (46), Batch(1600/2189), loss: 0.203578, imid loss: 0.053660, cmid loss: 0.149918
Epoch (46), Batch(1800/2189), loss: 0.202619, imid loss: 0.053439, cmid loss: 0.149180
Epoch (46), Batch(2000/2189), loss: 0.202369, imid loss: 0.053378, cmid loss: 0.148991
Train 46, loss: 0.202133
Linear Accuracy : 0.9043760129659644
Start training epoch: (47/100)
Epoch (47), Batch(0/2189), loss: 0.519773, imid loss: 0.253127, cmid loss: 0.266646
Epoch (47), Batch(200/2189), loss: 0.185708, imid loss: 0.051915, cmid loss: 0.133793
Epoch (47), Batch(400/2189), loss: 0.195667, imid loss: 0.054292, cmid loss: 0.141375
Epoch (47), Batch(600/2189), loss: 0.192992, imid loss: 0.053182, cmid loss: 0.139810
Epoch (47), Batch(800/2189), loss: 0.196299, imid loss: 0.054095, cmid loss: 0.142204
Epoch (47), Batch(1000/2189), loss: 0.195724, imid loss: 0.053956, cmid loss: 0.141768
Epoch (47), Batch(1200/2189), loss: 0.195838, imid loss: 0.053872, cmid loss: 0.141966
Epoch (47), Batch(1400/2189), loss: 0.195428, imid loss: 0.053616, cmid loss: 0.141813
Epoch (47), Batch(1600/2189), loss: 0.195583, imid loss: 0.053260, cmid loss: 0.142323
Epoch (47), Batch(1800/2189), loss: 0.196486, imid loss: 0.053432, cmid loss: 0.143055
Epoch (47), Batch(2000/2189), loss: 0.194783, imid loss: 0.052624, cmid loss: 0.142159
Train 47, loss: 0.193957
Linear Accuracy : 0.9027552674230146
Start training epoch: (48/100)
Epoch (48), Batch(0/2189), loss: 0.153585, imid loss: 0.036385, cmid loss: 0.117200
Epoch (48), Batch(200/2189), loss: 0.204850, imid loss: 0.056701, cmid loss: 0.148150
Epoch (48), Batch(400/2189), loss: 0.202577, imid loss: 0.056751, cmid loss: 0.145826
Epoch (48), Batch(600/2189), loss: 0.197064, imid loss: 0.055391, cmid loss: 0.141674
Epoch (48), Batch(800/2189), loss: 0.192210, imid loss: 0.053802, cmid loss: 0.138408
Epoch (48), Batch(1000/2189), loss: 0.190373, imid loss: 0.053574, cmid loss: 0.136800
Epoch (48), Batch(1200/2189), loss: 0.188740, imid loss: 0.052092, cmid loss: 0.136648
Epoch (48), Batch(1400/2189), loss: 0.189375, imid loss: 0.052274, cmid loss: 0.137102
Epoch (48), Batch(1600/2189), loss: 0.190723, imid loss: 0.052234, cmid loss: 0.138489
Epoch (48), Batch(1800/2189), loss: 0.189916, imid loss: 0.051726, cmid loss: 0.138189
Epoch (48), Batch(2000/2189), loss: 0.190476, imid loss: 0.051739, cmid loss: 0.138736
Train 48, loss: 0.188914
Linear Accuracy : 0.9023500810372771
Start training epoch: (49/100)
Epoch (49), Batch(0/2189), loss: 0.272091, imid loss: 0.071043, cmid loss: 0.201048
Epoch (49), Batch(200/2189), loss: 0.188701, imid loss: 0.051739, cmid loss: 0.136962
Epoch (49), Batch(400/2189), loss: 0.186640, imid loss: 0.050427, cmid loss: 0.136212
Epoch (49), Batch(600/2189), loss: 0.187163, imid loss: 0.050645, cmid loss: 0.136518
Epoch (49), Batch(800/2189), loss: 0.185806, imid loss: 0.049817, cmid loss: 0.135989
Epoch (49), Batch(1000/2189), loss: 0.185807, imid loss: 0.049630, cmid loss: 0.136178
Epoch (49), Batch(1200/2189), loss: 0.187554, imid loss: 0.050345, cmid loss: 0.137209
Epoch (49), Batch(1400/2189), loss: 0.186466, imid loss: 0.050339, cmid loss: 0.136127
Epoch (49), Batch(1600/2189), loss: 0.187370, imid loss: 0.050578, cmid loss: 0.136791
Epoch (49), Batch(1800/2189), loss: 0.188717, imid loss: 0.050799, cmid loss: 0.137918
Epoch (49), Batch(2000/2189), loss: 0.189497, imid loss: 0.051082, cmid loss: 0.138415
Train 49, loss: 0.189004
Linear Accuracy : 0.8978930307941653
Start training epoch: (50/100)
Epoch (50), Batch(0/2189), loss: 0.104848, imid loss: 0.040032, cmid loss: 0.064816
Epoch (50), Batch(200/2189), loss: 0.193355, imid loss: 0.057239, cmid loss: 0.136117
Epoch (50), Batch(400/2189), loss: 0.191424, imid loss: 0.054072, cmid loss: 0.137351
Epoch (50), Batch(600/2189), loss: 0.191385, imid loss: 0.053499, cmid loss: 0.137886
Epoch (50), Batch(800/2189), loss: 0.190597, imid loss: 0.053844, cmid loss: 0.136753
Epoch (50), Batch(1000/2189), loss: 0.188336, imid loss: 0.052658, cmid loss: 0.135678
Epoch (50), Batch(1200/2189), loss: 0.188055, imid loss: 0.052550, cmid loss: 0.135506
Epoch (50), Batch(1400/2189), loss: 0.187518, imid loss: 0.052803, cmid loss: 0.134715
Epoch (50), Batch(1600/2189), loss: 0.185873, imid loss: 0.051900, cmid loss: 0.133973
Epoch (50), Batch(1800/2189), loss: 0.185315, imid loss: 0.051605, cmid loss: 0.133711
Epoch (50), Batch(2000/2189), loss: 0.182636, imid loss: 0.050723, cmid loss: 0.131913
Train 50, loss: 0.182569
Linear Accuracy : 0.9019448946515397
==> Saving...
Start training epoch: (51/100)
Epoch (51), Batch(0/2189), loss: 0.054834, imid loss: 0.022068, cmid loss: 0.032766
Epoch (51), Batch(200/2189), loss: 0.179584, imid loss: 0.046436, cmid loss: 0.133148
Epoch (51), Batch(400/2189), loss: 0.177450, imid loss: 0.046303, cmid loss: 0.131147
Epoch (51), Batch(600/2189), loss: 0.180866, imid loss: 0.049024, cmid loss: 0.131842
Epoch (51), Batch(800/2189), loss: 0.180039, imid loss: 0.048746, cmid loss: 0.131293
Epoch (51), Batch(1000/2189), loss: 0.180567, imid loss: 0.048560, cmid loss: 0.132006
Epoch (51), Batch(1200/2189), loss: 0.182728, imid loss: 0.049386, cmid loss: 0.133342
Epoch (51), Batch(1400/2189), loss: 0.183462, imid loss: 0.050201, cmid loss: 0.133261
Epoch (51), Batch(1600/2189), loss: 0.183317, imid loss: 0.050511, cmid loss: 0.132805
Epoch (51), Batch(1800/2189), loss: 0.184060, imid loss: 0.050656, cmid loss: 0.133405
Epoch (51), Batch(2000/2189), loss: 0.183922, imid loss: 0.050356, cmid loss: 0.133566
Train 51, loss: 0.184782
Linear Accuracy : 0.8974878444084279
Start training epoch: (52/100)
Epoch (52), Batch(0/2189), loss: 0.089689, imid loss: 0.021967, cmid loss: 0.067722
Epoch (52), Batch(200/2189), loss: 0.170521, imid loss: 0.045015, cmid loss: 0.125505
Epoch (52), Batch(400/2189), loss: 0.179490, imid loss: 0.046818, cmid loss: 0.132673
Epoch (52), Batch(600/2189), loss: 0.176450, imid loss: 0.046278, cmid loss: 0.130172
Epoch (52), Batch(800/2189), loss: 0.180191, imid loss: 0.048033, cmid loss: 0.132158
Epoch (52), Batch(1000/2189), loss: 0.179483, imid loss: 0.049031, cmid loss: 0.130452
Epoch (52), Batch(1200/2189), loss: 0.178937, imid loss: 0.049058, cmid loss: 0.129879
Epoch (52), Batch(1400/2189), loss: 0.179559, imid loss: 0.049058, cmid loss: 0.130501
Epoch (52), Batch(1600/2189), loss: 0.177536, imid loss: 0.048101, cmid loss: 0.129435
Epoch (52), Batch(1800/2189), loss: 0.177120, imid loss: 0.047719, cmid loss: 0.129401
Epoch (52), Batch(2000/2189), loss: 0.175249, imid loss: 0.047025, cmid loss: 0.128224
Train 52, loss: 0.174468
Linear Accuracy : 0.8905996758508914
Start training epoch: (53/100)
Epoch (53), Batch(0/2189), loss: 0.090414, imid loss: 0.014235, cmid loss: 0.076179
Epoch (53), Batch(200/2189), loss: 0.172407, imid loss: 0.043865, cmid loss: 0.128542
Epoch (53), Batch(400/2189), loss: 0.175195, imid loss: 0.047379, cmid loss: 0.127816
Epoch (53), Batch(600/2189), loss: 0.179156, imid loss: 0.048717, cmid loss: 0.130439
Epoch (53), Batch(800/2189), loss: 0.177846, imid loss: 0.049035, cmid loss: 0.128812
Epoch (53), Batch(1000/2189), loss: 0.177229, imid loss: 0.048485, cmid loss: 0.128744
Epoch (53), Batch(1200/2189), loss: 0.177772, imid loss: 0.048636, cmid loss: 0.129136
Epoch (53), Batch(1400/2189), loss: 0.177534, imid loss: 0.048609, cmid loss: 0.128925
Epoch (53), Batch(1600/2189), loss: 0.178883, imid loss: 0.049018, cmid loss: 0.129865
Epoch (53), Batch(1800/2189), loss: 0.178301, imid loss: 0.048304, cmid loss: 0.129997
Epoch (53), Batch(2000/2189), loss: 0.176825, imid loss: 0.047887, cmid loss: 0.128938
Train 53, loss: 0.175812
Linear Accuracy : 0.9072123176661264
==> Saving Best Model...
Start training epoch: (54/100)
Epoch (54), Batch(0/2189), loss: 0.199415, imid loss: 0.116964, cmid loss: 0.082451
Epoch (54), Batch(200/2189), loss: 0.164346, imid loss: 0.047081, cmid loss: 0.117265
Epoch (54), Batch(400/2189), loss: 0.168035, imid loss: 0.047043, cmid loss: 0.120992
Epoch (54), Batch(600/2189), loss: 0.170554, imid loss: 0.045751, cmid loss: 0.124802
Epoch (54), Batch(800/2189), loss: 0.172107, imid loss: 0.046788, cmid loss: 0.125319
Epoch (54), Batch(1000/2189), loss: 0.170936, imid loss: 0.046544, cmid loss: 0.124391
Epoch (54), Batch(1200/2189), loss: 0.169718, imid loss: 0.045684, cmid loss: 0.124034
Epoch (54), Batch(1400/2189), loss: 0.167807, imid loss: 0.044907, cmid loss: 0.122900
Epoch (54), Batch(1600/2189), loss: 0.167190, imid loss: 0.044957, cmid loss: 0.122233
Epoch (54), Batch(1800/2189), loss: 0.167189, imid loss: 0.045000, cmid loss: 0.122189
Epoch (54), Batch(2000/2189), loss: 0.167118, imid loss: 0.045051, cmid loss: 0.122067
Train 54, loss: 0.167924
Linear Accuracy : 0.8958670988654781
Start training epoch: (55/100)
Epoch (55), Batch(0/2189), loss: 0.121179, imid loss: 0.031007, cmid loss: 0.090172
Epoch (55), Batch(200/2189), loss: 0.163314, imid loss: 0.043596, cmid loss: 0.119717
Epoch (55), Batch(400/2189), loss: 0.160928, imid loss: 0.043183, cmid loss: 0.117744
Epoch (55), Batch(600/2189), loss: 0.167173, imid loss: 0.045119, cmid loss: 0.122054
Epoch (55), Batch(800/2189), loss: 0.171554, imid loss: 0.046076, cmid loss: 0.125478
Epoch (55), Batch(1000/2189), loss: 0.171433, imid loss: 0.046787, cmid loss: 0.124646
Epoch (55), Batch(1200/2189), loss: 0.171244, imid loss: 0.046972, cmid loss: 0.124272
Epoch (55), Batch(1400/2189), loss: 0.168164, imid loss: 0.046255, cmid loss: 0.121909
Epoch (55), Batch(1600/2189), loss: 0.169682, imid loss: 0.046301, cmid loss: 0.123381
Epoch (55), Batch(1800/2189), loss: 0.171176, imid loss: 0.046789, cmid loss: 0.124388
Epoch (55), Batch(2000/2189), loss: 0.170230, imid loss: 0.046543, cmid loss: 0.123687
Train 55, loss: 0.171205
Linear Accuracy : 0.8978930307941653
==> Saving...
Start training epoch: (56/100)
Epoch (56), Batch(0/2189), loss: 0.426725, imid loss: 0.205171, cmid loss: 0.221553
Epoch (56), Batch(200/2189), loss: 0.167037, imid loss: 0.048120, cmid loss: 0.118917
Epoch (56), Batch(400/2189), loss: 0.162854, imid loss: 0.046601, cmid loss: 0.116252
Epoch (56), Batch(600/2189), loss: 0.161279, imid loss: 0.045256, cmid loss: 0.116023
Epoch (56), Batch(800/2189), loss: 0.160995, imid loss: 0.044993, cmid loss: 0.116002
Epoch (56), Batch(1000/2189), loss: 0.160966, imid loss: 0.044852, cmid loss: 0.116115
Epoch (56), Batch(1200/2189), loss: 0.161078, imid loss: 0.044861, cmid loss: 0.116217
Epoch (56), Batch(1400/2189), loss: 0.162171, imid loss: 0.044210, cmid loss: 0.117961
Epoch (56), Batch(1600/2189), loss: 0.161627, imid loss: 0.043650, cmid loss: 0.117977
Epoch (56), Batch(1800/2189), loss: 0.161797, imid loss: 0.043301, cmid loss: 0.118496
Epoch (56), Batch(2000/2189), loss: 0.163348, imid loss: 0.043987, cmid loss: 0.119361
Train 56, loss: 0.164033
Linear Accuracy : 0.8974878444084279
Start training epoch: (57/100)
Epoch (57), Batch(0/2189), loss: 0.080647, imid loss: 0.013625, cmid loss: 0.067022
Epoch (57), Batch(200/2189), loss: 0.153785, imid loss: 0.043270, cmid loss: 0.110514
Epoch (57), Batch(400/2189), loss: 0.157349, imid loss: 0.044116, cmid loss: 0.113233
Epoch (57), Batch(600/2189), loss: 0.160593, imid loss: 0.043216, cmid loss: 0.117377
Epoch (57), Batch(800/2189), loss: 0.162723, imid loss: 0.044537, cmid loss: 0.118186
Epoch (57), Batch(1000/2189), loss: 0.160148, imid loss: 0.043952, cmid loss: 0.116196
Epoch (57), Batch(1200/2189), loss: 0.158887, imid loss: 0.043224, cmid loss: 0.115663
Epoch (57), Batch(1400/2189), loss: 0.158104, imid loss: 0.042991, cmid loss: 0.115113
Epoch (57), Batch(1600/2189), loss: 0.157521, imid loss: 0.042506, cmid loss: 0.115015
Epoch (57), Batch(1800/2189), loss: 0.157021, imid loss: 0.042322, cmid loss: 0.114700
Epoch (57), Batch(2000/2189), loss: 0.155583, imid loss: 0.041680, cmid loss: 0.113903
Train 57, loss: 0.156878
Linear Accuracy : 0.8954619124797407
Start training epoch: (58/100)
Epoch (58), Batch(0/2189), loss: 0.254404, imid loss: 0.044051, cmid loss: 0.210353
Epoch (58), Batch(200/2189), loss: 0.160589, imid loss: 0.041964, cmid loss: 0.118625
Epoch (58), Batch(400/2189), loss: 0.154857, imid loss: 0.042976, cmid loss: 0.111881
Epoch (58), Batch(600/2189), loss: 0.157549, imid loss: 0.043790, cmid loss: 0.113759
Epoch (58), Batch(800/2189), loss: 0.155821, imid loss: 0.043261, cmid loss: 0.112560
Epoch (58), Batch(1000/2189), loss: 0.156268, imid loss: 0.043350, cmid loss: 0.112918
Epoch (58), Batch(1200/2189), loss: 0.155818, imid loss: 0.042795, cmid loss: 0.113023
Epoch (58), Batch(1400/2189), loss: 0.155604, imid loss: 0.042312, cmid loss: 0.113292
Epoch (58), Batch(1600/2189), loss: 0.154874, imid loss: 0.042090, cmid loss: 0.112784
Epoch (58), Batch(1800/2189), loss: 0.154275, imid loss: 0.041807, cmid loss: 0.112467
Epoch (58), Batch(2000/2189), loss: 0.153575, imid loss: 0.041557, cmid loss: 0.112018
Train 58, loss: 0.154467
Linear Accuracy : 0.9035656401944895
Start training epoch: (59/100)
Epoch (59), Batch(0/2189), loss: 0.123615, imid loss: 0.061498, cmid loss: 0.062117
Epoch (59), Batch(200/2189), loss: 0.151210, imid loss: 0.044283, cmid loss: 0.106927
Epoch (59), Batch(400/2189), loss: 0.153550, imid loss: 0.043519, cmid loss: 0.110032
Epoch (59), Batch(600/2189), loss: 0.151457, imid loss: 0.042063, cmid loss: 0.109394
Epoch (59), Batch(800/2189), loss: 0.150796, imid loss: 0.041631, cmid loss: 0.109165
Epoch (59), Batch(1000/2189), loss: 0.152252, imid loss: 0.042336, cmid loss: 0.109916
Epoch (59), Batch(1200/2189), loss: 0.153491, imid loss: 0.042637, cmid loss: 0.110854
Epoch (59), Batch(1400/2189), loss: 0.152355, imid loss: 0.042582, cmid loss: 0.109773
Epoch (59), Batch(1600/2189), loss: 0.151527, imid loss: 0.042341, cmid loss: 0.109186
Epoch (59), Batch(1800/2189), loss: 0.151803, imid loss: 0.042268, cmid loss: 0.109536
Epoch (59), Batch(2000/2189), loss: 0.153651, imid loss: 0.042492, cmid loss: 0.111159
Train 59, loss: 0.152958
Linear Accuracy : 0.8999189627228525
Start training epoch: (60/100)
Epoch (60), Batch(0/2189), loss: 0.199968, imid loss: 0.085713, cmid loss: 0.114254
Epoch (60), Batch(200/2189), loss: 0.146694, imid loss: 0.044160, cmid loss: 0.102535
Epoch (60), Batch(400/2189), loss: 0.143338, imid loss: 0.041681, cmid loss: 0.101657
Epoch (60), Batch(600/2189), loss: 0.146740, imid loss: 0.041433, cmid loss: 0.105307
Epoch (60), Batch(800/2189), loss: 0.148052, imid loss: 0.041425, cmid loss: 0.106627
Epoch (60), Batch(1000/2189), loss: 0.148682, imid loss: 0.041352, cmid loss: 0.107331
Epoch (60), Batch(1200/2189), loss: 0.150474, imid loss: 0.042043, cmid loss: 0.108431
Epoch (60), Batch(1400/2189), loss: 0.151898, imid loss: 0.042358, cmid loss: 0.109540
Epoch (60), Batch(1600/2189), loss: 0.152767, imid loss: 0.042079, cmid loss: 0.110687
Epoch (60), Batch(1800/2189), loss: 0.154410, imid loss: 0.042560, cmid loss: 0.111850
Epoch (60), Batch(2000/2189), loss: 0.153812, imid loss: 0.042406, cmid loss: 0.111406
Train 60, loss: 0.154001
Linear Accuracy : 0.9015397082658023
==> Saving...
Start training epoch: (61/100)
Epoch (61), Batch(0/2189), loss: 0.171938, imid loss: 0.019690, cmid loss: 0.152248
Epoch (61), Batch(200/2189), loss: 0.155580, imid loss: 0.039989, cmid loss: 0.115592
Epoch (61), Batch(400/2189), loss: 0.155895, imid loss: 0.042142, cmid loss: 0.113753
Epoch (61), Batch(600/2189), loss: 0.154986, imid loss: 0.042040, cmid loss: 0.112946
Epoch (61), Batch(800/2189), loss: 0.154631, imid loss: 0.041896, cmid loss: 0.112735
Epoch (61), Batch(1000/2189), loss: 0.153111, imid loss: 0.042562, cmid loss: 0.110548
Epoch (61), Batch(1200/2189), loss: 0.150990, imid loss: 0.042422, cmid loss: 0.108568
Epoch (61), Batch(1400/2189), loss: 0.150168, imid loss: 0.042013, cmid loss: 0.108155
Epoch (61), Batch(1600/2189), loss: 0.150573, imid loss: 0.042026, cmid loss: 0.108546
Epoch (61), Batch(1800/2189), loss: 0.149840, imid loss: 0.041738, cmid loss: 0.108101
Epoch (61), Batch(2000/2189), loss: 0.149266, imid loss: 0.041147, cmid loss: 0.108120
Train 61, loss: 0.148115
Linear Accuracy : 0.8962722852512156
Start training epoch: (62/100)
Epoch (62), Batch(0/2189), loss: 0.070573, imid loss: 0.017235, cmid loss: 0.053337
Epoch (62), Batch(200/2189), loss: 0.144554, imid loss: 0.037238, cmid loss: 0.107316
Epoch (62), Batch(400/2189), loss: 0.156975, imid loss: 0.042887, cmid loss: 0.114088
Epoch (62), Batch(600/2189), loss: 0.151680, imid loss: 0.041614, cmid loss: 0.110066
Epoch (62), Batch(800/2189), loss: 0.148343, imid loss: 0.040626, cmid loss: 0.107717
Epoch (62), Batch(1000/2189), loss: 0.144549, imid loss: 0.039372, cmid loss: 0.105177
Epoch (62), Batch(1200/2189), loss: 0.146342, imid loss: 0.040043, cmid loss: 0.106300
Epoch (62), Batch(1400/2189), loss: 0.144269, imid loss: 0.039483, cmid loss: 0.104785
Epoch (62), Batch(1600/2189), loss: 0.144993, imid loss: 0.039258, cmid loss: 0.105736
Epoch (62), Batch(1800/2189), loss: 0.144191, imid loss: 0.039098, cmid loss: 0.105093
Epoch (62), Batch(2000/2189), loss: 0.144976, imid loss: 0.039512, cmid loss: 0.105464
Train 62, loss: 0.145267
Linear Accuracy : 0.9023500810372771
Start training epoch: (63/100)
Epoch (63), Batch(0/2189), loss: 0.028262, imid loss: 0.012431, cmid loss: 0.015831
Epoch (63), Batch(200/2189), loss: 0.153784, imid loss: 0.043715, cmid loss: 0.110069
Epoch (63), Batch(400/2189), loss: 0.153041, imid loss: 0.043154, cmid loss: 0.109887
Epoch (63), Batch(600/2189), loss: 0.149282, imid loss: 0.041766, cmid loss: 0.107515
Epoch (63), Batch(800/2189), loss: 0.148474, imid loss: 0.040813, cmid loss: 0.107661
Epoch (63), Batch(1000/2189), loss: 0.150440, imid loss: 0.040904, cmid loss: 0.109536
Epoch (63), Batch(1200/2189), loss: 0.147520, imid loss: 0.040393, cmid loss: 0.107127
Epoch (63), Batch(1400/2189), loss: 0.146586, imid loss: 0.040402, cmid loss: 0.106183
Epoch (63), Batch(1600/2189), loss: 0.146350, imid loss: 0.040282, cmid loss: 0.106067
Epoch (63), Batch(1800/2189), loss: 0.145694, imid loss: 0.040114, cmid loss: 0.105580
Epoch (63), Batch(2000/2189), loss: 0.144607, imid loss: 0.039713, cmid loss: 0.104893
Train 63, loss: 0.145178
Linear Accuracy : 0.9023500810372771
Start training epoch: (64/100)
Epoch (64), Batch(0/2189), loss: 0.061130, imid loss: 0.019055, cmid loss: 0.042075
Epoch (64), Batch(200/2189), loss: 0.135124, imid loss: 0.038570, cmid loss: 0.096555
Epoch (64), Batch(400/2189), loss: 0.142994, imid loss: 0.042196, cmid loss: 0.100798
Epoch (64), Batch(600/2189), loss: 0.141947, imid loss: 0.040534, cmid loss: 0.101413
Epoch (64), Batch(800/2189), loss: 0.142642, imid loss: 0.040551, cmid loss: 0.102091
Epoch (64), Batch(1000/2189), loss: 0.143081, imid loss: 0.040507, cmid loss: 0.102575
Epoch (64), Batch(1200/2189), loss: 0.144441, imid loss: 0.040586, cmid loss: 0.103855
Epoch (64), Batch(1400/2189), loss: 0.143044, imid loss: 0.040479, cmid loss: 0.102565
Epoch (64), Batch(1600/2189), loss: 0.142696, imid loss: 0.040457, cmid loss: 0.102240
Epoch (64), Batch(1800/2189), loss: 0.143209, imid loss: 0.040758, cmid loss: 0.102451
Epoch (64), Batch(2000/2189), loss: 0.143082, imid loss: 0.040849, cmid loss: 0.102233
Train 64, loss: 0.142087
Linear Accuracy : 0.9039708265802269
Start training epoch: (65/100)
Epoch (65), Batch(0/2189), loss: 0.045051, imid loss: 0.009419, cmid loss: 0.035632
Epoch (65), Batch(200/2189), loss: 0.124407, imid loss: 0.036751, cmid loss: 0.087656
Epoch (65), Batch(400/2189), loss: 0.136293, imid loss: 0.039549, cmid loss: 0.096744
Epoch (65), Batch(600/2189), loss: 0.131122, imid loss: 0.037143, cmid loss: 0.093978
Epoch (65), Batch(800/2189), loss: 0.134107, imid loss: 0.037224, cmid loss: 0.096883
Epoch (65), Batch(1000/2189), loss: 0.134737, imid loss: 0.037088, cmid loss: 0.097649
Epoch (65), Batch(1200/2189), loss: 0.136394, imid loss: 0.037713, cmid loss: 0.098682
Epoch (65), Batch(1400/2189), loss: 0.137180, imid loss: 0.037515, cmid loss: 0.099665
Epoch (65), Batch(1600/2189), loss: 0.138409, imid loss: 0.037381, cmid loss: 0.101028
Epoch (65), Batch(1800/2189), loss: 0.139961, imid loss: 0.037908, cmid loss: 0.102053
Epoch (65), Batch(2000/2189), loss: 0.138722, imid loss: 0.037778, cmid loss: 0.100944
Train 65, loss: 0.138157
Linear Accuracy : 0.8987034035656402
==> Saving...
Start training epoch: (66/100)
Epoch (66), Batch(0/2189), loss: 0.048732, imid loss: 0.019555, cmid loss: 0.029177
Epoch (66), Batch(200/2189), loss: 0.130566, imid loss: 0.035402, cmid loss: 0.095163
Epoch (66), Batch(400/2189), loss: 0.134900, imid loss: 0.037047, cmid loss: 0.097852
Epoch (66), Batch(600/2189), loss: 0.134975, imid loss: 0.036716, cmid loss: 0.098259
Epoch (66), Batch(800/2189), loss: 0.134688, imid loss: 0.036008, cmid loss: 0.098680
Epoch (66), Batch(1000/2189), loss: 0.137162, imid loss: 0.037282, cmid loss: 0.099880
Epoch (66), Batch(1200/2189), loss: 0.137171, imid loss: 0.037778, cmid loss: 0.099393
Epoch (66), Batch(1400/2189), loss: 0.138525, imid loss: 0.038177, cmid loss: 0.100347
Epoch (66), Batch(1600/2189), loss: 0.138262, imid loss: 0.037993, cmid loss: 0.100269
Epoch (66), Batch(1800/2189), loss: 0.139086, imid loss: 0.037958, cmid loss: 0.101129
Epoch (66), Batch(2000/2189), loss: 0.138327, imid loss: 0.037693, cmid loss: 0.100633
Train 66, loss: 0.138202
Linear Accuracy : 0.896677471636953
Start training epoch: (67/100)
Epoch (67), Batch(0/2189), loss: 0.101384, imid loss: 0.027225, cmid loss: 0.074158
Epoch (67), Batch(200/2189), loss: 0.143271, imid loss: 0.038231, cmid loss: 0.105040
Epoch (67), Batch(400/2189), loss: 0.141734, imid loss: 0.039307, cmid loss: 0.102427
Epoch (67), Batch(600/2189), loss: 0.138571, imid loss: 0.039148, cmid loss: 0.099423
Epoch (67), Batch(800/2189), loss: 0.136123, imid loss: 0.038728, cmid loss: 0.097395
Epoch (67), Batch(1000/2189), loss: 0.134590, imid loss: 0.037540, cmid loss: 0.097050
Epoch (67), Batch(1200/2189), loss: 0.132569, imid loss: 0.036826, cmid loss: 0.095743
Epoch (67), Batch(1400/2189), loss: 0.133195, imid loss: 0.036795, cmid loss: 0.096401
Epoch (67), Batch(1600/2189), loss: 0.132138, imid loss: 0.036427, cmid loss: 0.095712
Epoch (67), Batch(1800/2189), loss: 0.131235, imid loss: 0.036117, cmid loss: 0.095118
Epoch (67), Batch(2000/2189), loss: 0.131877, imid loss: 0.036618, cmid loss: 0.095259
Train 67, loss: 0.131648
Linear Accuracy : 0.9039708265802269
Start training epoch: (68/100)
Epoch (68), Batch(0/2189), loss: 0.160786, imid loss: 0.068053, cmid loss: 0.092733
Epoch (68), Batch(200/2189), loss: 0.129312, imid loss: 0.037530, cmid loss: 0.091781
Epoch (68), Batch(400/2189), loss: 0.124736, imid loss: 0.035777, cmid loss: 0.088959
Epoch (68), Batch(600/2189), loss: 0.125718, imid loss: 0.036120, cmid loss: 0.089597
Epoch (68), Batch(800/2189), loss: 0.129538, imid loss: 0.037025, cmid loss: 0.092513
Epoch (68), Batch(1000/2189), loss: 0.128549, imid loss: 0.036182, cmid loss: 0.092367
Epoch (68), Batch(1200/2189), loss: 0.129485, imid loss: 0.036467, cmid loss: 0.093018
Epoch (68), Batch(1400/2189), loss: 0.129144, imid loss: 0.036371, cmid loss: 0.092773
Epoch (68), Batch(1600/2189), loss: 0.130059, imid loss: 0.036500, cmid loss: 0.093559
Epoch (68), Batch(1800/2189), loss: 0.129148, imid loss: 0.036331, cmid loss: 0.092816
Epoch (68), Batch(2000/2189), loss: 0.128481, imid loss: 0.036256, cmid loss: 0.092224
Train 68, loss: 0.128728
Linear Accuracy : 0.896677471636953
Start training epoch: (69/100)
Epoch (69), Batch(0/2189), loss: 0.132565, imid loss: 0.041029, cmid loss: 0.091536
Epoch (69), Batch(200/2189), loss: 0.124404, imid loss: 0.034116, cmid loss: 0.090289
Epoch (69), Batch(400/2189), loss: 0.128155, imid loss: 0.035493, cmid loss: 0.092662
Epoch (69), Batch(600/2189), loss: 0.128817, imid loss: 0.035991, cmid loss: 0.092826
Epoch (69), Batch(800/2189), loss: 0.129342, imid loss: 0.035879, cmid loss: 0.093462
Epoch (69), Batch(1000/2189), loss: 0.129284, imid loss: 0.035923, cmid loss: 0.093361
Epoch (69), Batch(1200/2189), loss: 0.126999, imid loss: 0.035144, cmid loss: 0.091855
Epoch (69), Batch(1400/2189), loss: 0.126836, imid loss: 0.035365, cmid loss: 0.091471
Epoch (69), Batch(1600/2189), loss: 0.127035, imid loss: 0.035487, cmid loss: 0.091548
Epoch (69), Batch(1800/2189), loss: 0.127886, imid loss: 0.035631, cmid loss: 0.092255
Epoch (69), Batch(2000/2189), loss: 0.128371, imid loss: 0.035678, cmid loss: 0.092693
Train 69, loss: 0.127120
Linear Accuracy : 0.9019448946515397
Start training epoch: (70/100)
Epoch (70), Batch(0/2189), loss: 0.059968, imid loss: 0.010887, cmid loss: 0.049081
Epoch (70), Batch(200/2189), loss: 0.124303, imid loss: 0.033735, cmid loss: 0.090567
Epoch (70), Batch(400/2189), loss: 0.125038, imid loss: 0.033903, cmid loss: 0.091135
Epoch (70), Batch(600/2189), loss: 0.122301, imid loss: 0.033044, cmid loss: 0.089257
Epoch (70), Batch(800/2189), loss: 0.126497, imid loss: 0.034362, cmid loss: 0.092135
Epoch (70), Batch(1000/2189), loss: 0.126144, imid loss: 0.034583, cmid loss: 0.091560
Epoch (70), Batch(1200/2189), loss: 0.129028, imid loss: 0.035467, cmid loss: 0.093561
Epoch (70), Batch(1400/2189), loss: 0.128200, imid loss: 0.035461, cmid loss: 0.092739
Epoch (70), Batch(1600/2189), loss: 0.127751, imid loss: 0.035278, cmid loss: 0.092473
Epoch (70), Batch(1800/2189), loss: 0.126531, imid loss: 0.034892, cmid loss: 0.091640
Epoch (70), Batch(2000/2189), loss: 0.126092, imid loss: 0.034824, cmid loss: 0.091269
Train 70, loss: 0.125842
Linear Accuracy : 0.8987034035656402
==> Saving...
Start training epoch: (71/100)
Epoch (71), Batch(0/2189), loss: 0.116269, imid loss: 0.017985, cmid loss: 0.098284
Epoch (71), Batch(200/2189), loss: 0.110836, imid loss: 0.033719, cmid loss: 0.077118
Epoch (71), Batch(400/2189), loss: 0.119822, imid loss: 0.034968, cmid loss: 0.084855
Epoch (71), Batch(600/2189), loss: 0.119534, imid loss: 0.033730, cmid loss: 0.085804
Epoch (71), Batch(800/2189), loss: 0.123296, imid loss: 0.034855, cmid loss: 0.088442
Epoch (71), Batch(1000/2189), loss: 0.123638, imid loss: 0.035045, cmid loss: 0.088593
Epoch (71), Batch(1200/2189), loss: 0.125185, imid loss: 0.035242, cmid loss: 0.089943
Epoch (71), Batch(1400/2189), loss: 0.123710, imid loss: 0.034568, cmid loss: 0.089142
Epoch (71), Batch(1600/2189), loss: 0.123828, imid loss: 0.034522, cmid loss: 0.089306
Epoch (71), Batch(1800/2189), loss: 0.124457, imid loss: 0.034644, cmid loss: 0.089813
Epoch (71), Batch(2000/2189), loss: 0.125018, imid loss: 0.034765, cmid loss: 0.090254
Train 71, loss: 0.124558
Linear Accuracy : 0.8982982171799028
Start training epoch: (72/100)
Epoch (72), Batch(0/2189), loss: 0.044453, imid loss: 0.013128, cmid loss: 0.031325
Epoch (72), Batch(200/2189), loss: 0.110313, imid loss: 0.031277, cmid loss: 0.079036
Epoch (72), Batch(400/2189), loss: 0.118492, imid loss: 0.032892, cmid loss: 0.085599
Epoch (72), Batch(600/2189), loss: 0.118733, imid loss: 0.032336, cmid loss: 0.086397
Epoch (72), Batch(800/2189), loss: 0.119083, imid loss: 0.032350, cmid loss: 0.086732
Epoch (72), Batch(1000/2189), loss: 0.119357, imid loss: 0.032289, cmid loss: 0.087068
Epoch (72), Batch(1200/2189), loss: 0.118474, imid loss: 0.032320, cmid loss: 0.086154
Epoch (72), Batch(1400/2189), loss: 0.119378, imid loss: 0.032617, cmid loss: 0.086761
Epoch (72), Batch(1600/2189), loss: 0.119422, imid loss: 0.032518, cmid loss: 0.086904
Epoch (72), Batch(1800/2189), loss: 0.118880, imid loss: 0.032274, cmid loss: 0.086606
Epoch (72), Batch(2000/2189), loss: 0.118251, imid loss: 0.031910, cmid loss: 0.086340
Train 72, loss: 0.118639
Linear Accuracy : 0.9003241491085899
Start training epoch: (73/100)
Epoch (73), Batch(0/2189), loss: 0.103640, imid loss: 0.012302, cmid loss: 0.091337
Epoch (73), Batch(200/2189), loss: 0.121229, imid loss: 0.035493, cmid loss: 0.085736
Epoch (73), Batch(400/2189), loss: 0.117954, imid loss: 0.033504, cmid loss: 0.084450
Epoch (73), Batch(600/2189), loss: 0.121386, imid loss: 0.034643, cmid loss: 0.086743
Epoch (73), Batch(800/2189), loss: 0.121015, imid loss: 0.034280, cmid loss: 0.086735
Epoch (73), Batch(1000/2189), loss: 0.120614, imid loss: 0.034132, cmid loss: 0.086481
Epoch (73), Batch(1200/2189), loss: 0.120680, imid loss: 0.034035, cmid loss: 0.086645
Epoch (73), Batch(1400/2189), loss: 0.120572, imid loss: 0.033875, cmid loss: 0.086698
Epoch (73), Batch(1600/2189), loss: 0.119906, imid loss: 0.033782, cmid loss: 0.086124
Epoch (73), Batch(1800/2189), loss: 0.120490, imid loss: 0.033858, cmid loss: 0.086632
Epoch (73), Batch(2000/2189), loss: 0.119178, imid loss: 0.033548, cmid loss: 0.085630
Train 73, loss: 0.118970
Linear Accuracy : 0.9039708265802269
Start training epoch: (74/100)
Epoch (74), Batch(0/2189), loss: 0.086104, imid loss: 0.021938, cmid loss: 0.064166
Epoch (74), Batch(200/2189), loss: 0.125822, imid loss: 0.032649, cmid loss: 0.093172
Epoch (74), Batch(400/2189), loss: 0.129031, imid loss: 0.034687, cmid loss: 0.094344
Epoch (74), Batch(600/2189), loss: 0.125310, imid loss: 0.033435, cmid loss: 0.091875
Epoch (74), Batch(800/2189), loss: 0.121790, imid loss: 0.032469, cmid loss: 0.089321
Epoch (74), Batch(1000/2189), loss: 0.122651, imid loss: 0.032747, cmid loss: 0.089904
Epoch (74), Batch(1200/2189), loss: 0.123047, imid loss: 0.033087, cmid loss: 0.089960
Epoch (74), Batch(1400/2189), loss: 0.122973, imid loss: 0.032788, cmid loss: 0.090186
Epoch (74), Batch(1600/2189), loss: 0.122068, imid loss: 0.032805, cmid loss: 0.089263
Epoch (74), Batch(1800/2189), loss: 0.121398, imid loss: 0.032815, cmid loss: 0.088583
Epoch (74), Batch(2000/2189), loss: 0.120868, imid loss: 0.032516, cmid loss: 0.088352
Train 74, loss: 0.120548
Linear Accuracy : 0.8970826580226904
Start training epoch: (75/100)
Epoch (75), Batch(0/2189), loss: 0.686292, imid loss: 0.171525, cmid loss: 0.514766
Epoch (75), Batch(200/2189), loss: 0.119272, imid loss: 0.031425, cmid loss: 0.087846
Epoch (75), Batch(400/2189), loss: 0.114835, imid loss: 0.030597, cmid loss: 0.084238
Epoch (75), Batch(600/2189), loss: 0.116761, imid loss: 0.031399, cmid loss: 0.085362
Epoch (75), Batch(800/2189), loss: 0.117155, imid loss: 0.031568, cmid loss: 0.085587
Epoch (75), Batch(1000/2189), loss: 0.115019, imid loss: 0.031416, cmid loss: 0.083602
Epoch (75), Batch(1200/2189), loss: 0.112665, imid loss: 0.031152, cmid loss: 0.081513
Epoch (75), Batch(1400/2189), loss: 0.113845, imid loss: 0.031329, cmid loss: 0.082516
Epoch (75), Batch(1600/2189), loss: 0.114364, imid loss: 0.031616, cmid loss: 0.082748
Epoch (75), Batch(1800/2189), loss: 0.114470, imid loss: 0.031596, cmid loss: 0.082873
Epoch (75), Batch(2000/2189), loss: 0.115154, imid loss: 0.031830, cmid loss: 0.083324
Train 75, loss: 0.114668
Linear Accuracy : 0.8999189627228525
==> Saving...
Start training epoch: (76/100)
Epoch (76), Batch(0/2189), loss: 0.062458, imid loss: 0.007957, cmid loss: 0.054501
Epoch (76), Batch(200/2189), loss: 0.105979, imid loss: 0.032331, cmid loss: 0.073647
Epoch (76), Batch(400/2189), loss: 0.105281, imid loss: 0.030877, cmid loss: 0.074405
Epoch (76), Batch(600/2189), loss: 0.106824, imid loss: 0.030432, cmid loss: 0.076392
Epoch (76), Batch(800/2189), loss: 0.107256, imid loss: 0.030266, cmid loss: 0.076991
Epoch (76), Batch(1000/2189), loss: 0.109703, imid loss: 0.030958, cmid loss: 0.078746
Epoch (76), Batch(1200/2189), loss: 0.112773, imid loss: 0.031780, cmid loss: 0.080993
Epoch (76), Batch(1400/2189), loss: 0.114083, imid loss: 0.031743, cmid loss: 0.082341
Epoch (76), Batch(1600/2189), loss: 0.112229, imid loss: 0.031269, cmid loss: 0.080960
Epoch (76), Batch(1800/2189), loss: 0.111606, imid loss: 0.031111, cmid loss: 0.080495
Epoch (76), Batch(2000/2189), loss: 0.110748, imid loss: 0.031089, cmid loss: 0.079658
Train 76, loss: 0.111230
Linear Accuracy : 0.8922204213938412
Start training epoch: (77/100)
Epoch (77), Batch(0/2189), loss: 0.119955, imid loss: 0.038612, cmid loss: 0.081343
Epoch (77), Batch(200/2189), loss: 0.105917, imid loss: 0.027377, cmid loss: 0.078540
Epoch (77), Batch(400/2189), loss: 0.105192, imid loss: 0.027242, cmid loss: 0.077950
Epoch (77), Batch(600/2189), loss: 0.106797, imid loss: 0.029049, cmid loss: 0.077748
Epoch (77), Batch(800/2189), loss: 0.110082, imid loss: 0.030020, cmid loss: 0.080063
Epoch (77), Batch(1000/2189), loss: 0.108945, imid loss: 0.030137, cmid loss: 0.078808
Epoch (77), Batch(1200/2189), loss: 0.107669, imid loss: 0.029460, cmid loss: 0.078209
Epoch (77), Batch(1400/2189), loss: 0.108482, imid loss: 0.030132, cmid loss: 0.078350
Epoch (77), Batch(1600/2189), loss: 0.109494, imid loss: 0.030478, cmid loss: 0.079016
Epoch (77), Batch(1800/2189), loss: 0.110450, imid loss: 0.030589, cmid loss: 0.079861
Epoch (77), Batch(2000/2189), loss: 0.110133, imid loss: 0.030560, cmid loss: 0.079573
Train 77, loss: 0.110063
Linear Accuracy : 0.8974878444084279
Start training epoch: (78/100)
Epoch (78), Batch(0/2189), loss: 0.078046, imid loss: 0.019092, cmid loss: 0.058954
Epoch (78), Batch(200/2189), loss: 0.124180, imid loss: 0.036272, cmid loss: 0.087909
Epoch (78), Batch(400/2189), loss: 0.117273, imid loss: 0.033112, cmid loss: 0.084160
Epoch (78), Batch(600/2189), loss: 0.112550, imid loss: 0.031873, cmid loss: 0.080677
Epoch (78), Batch(800/2189), loss: 0.112831, imid loss: 0.032106, cmid loss: 0.080725
Epoch (78), Batch(1000/2189), loss: 0.114259, imid loss: 0.032394, cmid loss: 0.081864
Epoch (78), Batch(1200/2189), loss: 0.112634, imid loss: 0.031666, cmid loss: 0.080968
Epoch (78), Batch(1400/2189), loss: 0.113450, imid loss: 0.031542, cmid loss: 0.081907
Epoch (78), Batch(1600/2189), loss: 0.112507, imid loss: 0.031280, cmid loss: 0.081227
Epoch (78), Batch(1800/2189), loss: 0.112255, imid loss: 0.031432, cmid loss: 0.080823
Epoch (78), Batch(2000/2189), loss: 0.111528, imid loss: 0.031221, cmid loss: 0.080307
Train 78, loss: 0.110797
Linear Accuracy : 0.8942463533225283
Start training epoch: (79/100)
Epoch (79), Batch(0/2189), loss: 0.033821, imid loss: 0.009517, cmid loss: 0.024304
Epoch (79), Batch(200/2189), loss: 0.107627, imid loss: 0.029492, cmid loss: 0.078135
Epoch (79), Batch(400/2189), loss: 0.110149, imid loss: 0.031040, cmid loss: 0.079109
Epoch (79), Batch(600/2189), loss: 0.106775, imid loss: 0.030662, cmid loss: 0.076113
Epoch (79), Batch(800/2189), loss: 0.104423, imid loss: 0.029905, cmid loss: 0.074518
Epoch (79), Batch(1000/2189), loss: 0.105879, imid loss: 0.030133, cmid loss: 0.075746
Epoch (79), Batch(1200/2189), loss: 0.106021, imid loss: 0.030094, cmid loss: 0.075927
Epoch (79), Batch(1400/2189), loss: 0.104843, imid loss: 0.029708, cmid loss: 0.075135
Epoch (79), Batch(1600/2189), loss: 0.104926, imid loss: 0.029924, cmid loss: 0.075002
Epoch (79), Batch(1800/2189), loss: 0.105448, imid loss: 0.030132, cmid loss: 0.075316
Epoch (79), Batch(2000/2189), loss: 0.106146, imid loss: 0.030174, cmid loss: 0.075972
Train 79, loss: 0.106917
Linear Accuracy : 0.8958670988654781
Start training epoch: (80/100)
Epoch (80), Batch(0/2189), loss: 0.158185, imid loss: 0.068859, cmid loss: 0.089325
Epoch (80), Batch(200/2189), loss: 0.105176, imid loss: 0.028220, cmid loss: 0.076956
Epoch (80), Batch(400/2189), loss: 0.103418, imid loss: 0.027549, cmid loss: 0.075870
Epoch (80), Batch(600/2189), loss: 0.106302, imid loss: 0.028682, cmid loss: 0.077620
Epoch (80), Batch(800/2189), loss: 0.106262, imid loss: 0.029083, cmid loss: 0.077179
Epoch (80), Batch(1000/2189), loss: 0.106409, imid loss: 0.029187, cmid loss: 0.077222
Epoch (80), Batch(1200/2189), loss: 0.108198, imid loss: 0.030091, cmid loss: 0.078107
Epoch (80), Batch(1400/2189), loss: 0.107472, imid loss: 0.029878, cmid loss: 0.077594
Epoch (80), Batch(1600/2189), loss: 0.107463, imid loss: 0.029996, cmid loss: 0.077466
Epoch (80), Batch(1800/2189), loss: 0.108131, imid loss: 0.030121, cmid loss: 0.078010
Epoch (80), Batch(2000/2189), loss: 0.108515, imid loss: 0.030215, cmid loss: 0.078300
Train 80, loss: 0.108615
Linear Accuracy : 0.8910048622366289
==> Saving...
Start training epoch: (81/100)
Epoch (81), Batch(0/2189), loss: 0.251503, imid loss: 0.078240, cmid loss: 0.173263
Epoch (81), Batch(200/2189), loss: 0.103816, imid loss: 0.029399, cmid loss: 0.074417
Epoch (81), Batch(400/2189), loss: 0.105517, imid loss: 0.030138, cmid loss: 0.075380
Epoch (81), Batch(600/2189), loss: 0.107042, imid loss: 0.030534, cmid loss: 0.076508
Epoch (81), Batch(800/2189), loss: 0.106828, imid loss: 0.029748, cmid loss: 0.077080
Epoch (81), Batch(1000/2189), loss: 0.107111, imid loss: 0.029763, cmid loss: 0.077348
Epoch (81), Batch(1200/2189), loss: 0.107972, imid loss: 0.030247, cmid loss: 0.077724
Epoch (81), Batch(1400/2189), loss: 0.107426, imid loss: 0.030099, cmid loss: 0.077326
Epoch (81), Batch(1600/2189), loss: 0.106291, imid loss: 0.029614, cmid loss: 0.076677
Epoch (81), Batch(1800/2189), loss: 0.107172, imid loss: 0.029867, cmid loss: 0.077305
Epoch (81), Batch(2000/2189), loss: 0.106490, imid loss: 0.029680, cmid loss: 0.076811
Train 81, loss: 0.106696
Linear Accuracy : 0.8978930307941653
Start training epoch: (82/100)
Epoch (82), Batch(0/2189), loss: 0.096415, imid loss: 0.037737, cmid loss: 0.058677
Epoch (82), Batch(200/2189), loss: 0.105408, imid loss: 0.029956, cmid loss: 0.075452
Epoch (82), Batch(400/2189), loss: 0.101788, imid loss: 0.029796, cmid loss: 0.071991
Epoch (82), Batch(600/2189), loss: 0.102823, imid loss: 0.029761, cmid loss: 0.073062
Epoch (82), Batch(800/2189), loss: 0.103253, imid loss: 0.029351, cmid loss: 0.073901
Epoch (82), Batch(1000/2189), loss: 0.102515, imid loss: 0.029215, cmid loss: 0.073299
Epoch (82), Batch(1200/2189), loss: 0.103647, imid loss: 0.029549, cmid loss: 0.074098
Epoch (82), Batch(1400/2189), loss: 0.104313, imid loss: 0.029894, cmid loss: 0.074419
Epoch (82), Batch(1600/2189), loss: 0.104138, imid loss: 0.030132, cmid loss: 0.074006
Epoch (82), Batch(1800/2189), loss: 0.103743, imid loss: 0.029677, cmid loss: 0.074066
Epoch (82), Batch(2000/2189), loss: 0.102948, imid loss: 0.029323, cmid loss: 0.073625
Train 82, loss: 0.102614
Linear Accuracy : 0.9015397082658023
Start training epoch: (83/100)
Epoch (83), Batch(0/2189), loss: 0.042686, imid loss: 0.013588, cmid loss: 0.029098
Epoch (83), Batch(200/2189), loss: 0.106673, imid loss: 0.028990, cmid loss: 0.077683
Epoch (83), Batch(400/2189), loss: 0.102694, imid loss: 0.028037, cmid loss: 0.074657
Epoch (83), Batch(600/2189), loss: 0.103376, imid loss: 0.028146, cmid loss: 0.075230
Epoch (83), Batch(800/2189), loss: 0.104702, imid loss: 0.028603, cmid loss: 0.076099
Epoch (83), Batch(1000/2189), loss: 0.104613, imid loss: 0.028525, cmid loss: 0.076088
Epoch (83), Batch(1200/2189), loss: 0.105089, imid loss: 0.029066, cmid loss: 0.076022
Epoch (83), Batch(1400/2189), loss: 0.104655, imid loss: 0.028881, cmid loss: 0.075775
Epoch (83), Batch(1600/2189), loss: 0.105015, imid loss: 0.028887, cmid loss: 0.076128
Epoch (83), Batch(1800/2189), loss: 0.106042, imid loss: 0.029334, cmid loss: 0.076708
Epoch (83), Batch(2000/2189), loss: 0.105775, imid loss: 0.029390, cmid loss: 0.076384
Train 83, loss: 0.105014
Linear Accuracy : 0.8978930307941653
Start training epoch: (84/100)
Epoch (84), Batch(0/2189), loss: 0.019292, imid loss: 0.006633, cmid loss: 0.012660
Epoch (84), Batch(200/2189), loss: 0.101199, imid loss: 0.028423, cmid loss: 0.072776
Epoch (84), Batch(400/2189), loss: 0.104008, imid loss: 0.029266, cmid loss: 0.074742
Epoch (84), Batch(600/2189), loss: 0.104715, imid loss: 0.029722, cmid loss: 0.074993
Epoch (84), Batch(800/2189), loss: 0.103146, imid loss: 0.029159, cmid loss: 0.073987
Epoch (84), Batch(1000/2189), loss: 0.103030, imid loss: 0.029078, cmid loss: 0.073952
Epoch (84), Batch(1200/2189), loss: 0.102155, imid loss: 0.028824, cmid loss: 0.073331
Epoch (84), Batch(1400/2189), loss: 0.102022, imid loss: 0.028295, cmid loss: 0.073727
Epoch (84), Batch(1600/2189), loss: 0.101490, imid loss: 0.028173, cmid loss: 0.073317
Epoch (84), Batch(1800/2189), loss: 0.100948, imid loss: 0.028027, cmid loss: 0.072921
Epoch (84), Batch(2000/2189), loss: 0.100213, imid loss: 0.027794, cmid loss: 0.072419
Train 84, loss: 0.099996
Linear Accuracy : 0.8950567260940032
Start training epoch: (85/100)
Epoch (85), Batch(0/2189), loss: 0.020271, imid loss: 0.006124, cmid loss: 0.014147
Epoch (85), Batch(200/2189), loss: 0.103259, imid loss: 0.029287, cmid loss: 0.073972
Epoch (85), Batch(400/2189), loss: 0.095538, imid loss: 0.026793, cmid loss: 0.068745
Epoch (85), Batch(600/2189), loss: 0.097023, imid loss: 0.027697, cmid loss: 0.069326
Epoch (85), Batch(800/2189), loss: 0.096772, imid loss: 0.027794, cmid loss: 0.068977
Epoch (85), Batch(1000/2189), loss: 0.097822, imid loss: 0.027810, cmid loss: 0.070012
Epoch (85), Batch(1200/2189), loss: 0.099665, imid loss: 0.028130, cmid loss: 0.071535
Epoch (85), Batch(1400/2189), loss: 0.099582, imid loss: 0.027868, cmid loss: 0.071715
Epoch (85), Batch(1600/2189), loss: 0.098723, imid loss: 0.027550, cmid loss: 0.071173
Epoch (85), Batch(1800/2189), loss: 0.099241, imid loss: 0.027685, cmid loss: 0.071556
Epoch (85), Batch(2000/2189), loss: 0.099505, imid loss: 0.027814, cmid loss: 0.071691
Train 85, loss: 0.099317
Linear Accuracy : 0.8962722852512156
==> Saving...
Start training epoch: (86/100)
Epoch (86), Batch(0/2189), loss: 0.061834, imid loss: 0.010112, cmid loss: 0.051722
Epoch (86), Batch(200/2189), loss: 0.098306, imid loss: 0.029303, cmid loss: 0.069003
Epoch (86), Batch(400/2189), loss: 0.095151, imid loss: 0.027879, cmid loss: 0.067272
Epoch (86), Batch(600/2189), loss: 0.098941, imid loss: 0.028922, cmid loss: 0.070019
Epoch (86), Batch(800/2189), loss: 0.100495, imid loss: 0.029451, cmid loss: 0.071044
Epoch (86), Batch(1000/2189), loss: 0.099582, imid loss: 0.029454, cmid loss: 0.070128
Epoch (86), Batch(1200/2189), loss: 0.100328, imid loss: 0.029460, cmid loss: 0.070868
Epoch (86), Batch(1400/2189), loss: 0.100337, imid loss: 0.029157, cmid loss: 0.071180
Epoch (86), Batch(1600/2189), loss: 0.101306, imid loss: 0.029425, cmid loss: 0.071881
Epoch (86), Batch(1800/2189), loss: 0.100492, imid loss: 0.028948, cmid loss: 0.071544
Epoch (86), Batch(2000/2189), loss: 0.101853, imid loss: 0.029265, cmid loss: 0.072588
Train 86, loss: 0.101436
Linear Accuracy : 0.8982982171799028
Start training epoch: (87/100)
Epoch (87), Batch(0/2189), loss: 0.088207, imid loss: 0.020043, cmid loss: 0.068164
Epoch (87), Batch(200/2189), loss: 0.095895, imid loss: 0.029831, cmid loss: 0.066064
Epoch (87), Batch(400/2189), loss: 0.096050, imid loss: 0.028630, cmid loss: 0.067420
Epoch (87), Batch(600/2189), loss: 0.094351, imid loss: 0.028141, cmid loss: 0.066209
Epoch (87), Batch(800/2189), loss: 0.096287, imid loss: 0.028052, cmid loss: 0.068235
Epoch (87), Batch(1000/2189), loss: 0.096314, imid loss: 0.028226, cmid loss: 0.068088
Epoch (87), Batch(1200/2189), loss: 0.097818, imid loss: 0.028406, cmid loss: 0.069412
Epoch (87), Batch(1400/2189), loss: 0.098912, imid loss: 0.028772, cmid loss: 0.070140
Epoch (87), Batch(1600/2189), loss: 0.099254, imid loss: 0.028974, cmid loss: 0.070280
Epoch (87), Batch(1800/2189), loss: 0.099752, imid loss: 0.028718, cmid loss: 0.071035
Epoch (87), Batch(2000/2189), loss: 0.099716, imid loss: 0.028794, cmid loss: 0.070921
Train 87, loss: 0.099308
Linear Accuracy : 0.8991085899513777
Start training epoch: (88/100)
Epoch (88), Batch(0/2189), loss: 0.291790, imid loss: 0.042756, cmid loss: 0.249033
Epoch (88), Batch(200/2189), loss: 0.100493, imid loss: 0.028639, cmid loss: 0.071854
Epoch (88), Batch(400/2189), loss: 0.097951, imid loss: 0.027118, cmid loss: 0.070833
Epoch (88), Batch(600/2189), loss: 0.098091, imid loss: 0.027351, cmid loss: 0.070740
Epoch (88), Batch(800/2189), loss: 0.098015, imid loss: 0.027610, cmid loss: 0.070405
Epoch (88), Batch(1000/2189), loss: 0.096266, imid loss: 0.027158, cmid loss: 0.069108
Epoch (88), Batch(1200/2189), loss: 0.096374, imid loss: 0.026985, cmid loss: 0.069389
Epoch (88), Batch(1400/2189), loss: 0.095695, imid loss: 0.026781, cmid loss: 0.068913
Epoch (88), Batch(1600/2189), loss: 0.096186, imid loss: 0.026655, cmid loss: 0.069531
Epoch (88), Batch(1800/2189), loss: 0.096218, imid loss: 0.026542, cmid loss: 0.069676
Epoch (88), Batch(2000/2189), loss: 0.096913, imid loss: 0.026563, cmid loss: 0.070350
Train 88, loss: 0.097002
Linear Accuracy : 0.8970826580226904
Start training epoch: (89/100)
Epoch (89), Batch(0/2189), loss: 0.070983, imid loss: 0.010610, cmid loss: 0.060372
Epoch (89), Batch(200/2189), loss: 0.094069, imid loss: 0.024937, cmid loss: 0.069133
Epoch (89), Batch(400/2189), loss: 0.095778, imid loss: 0.025568, cmid loss: 0.070210
Epoch (89), Batch(600/2189), loss: 0.096194, imid loss: 0.025680, cmid loss: 0.070514
Epoch (89), Batch(800/2189), loss: 0.096873, imid loss: 0.025885, cmid loss: 0.070988
Epoch (89), Batch(1000/2189), loss: 0.096246, imid loss: 0.025850, cmid loss: 0.070396
Epoch (89), Batch(1200/2189), loss: 0.096676, imid loss: 0.026163, cmid loss: 0.070513
Epoch (89), Batch(1400/2189), loss: 0.095500, imid loss: 0.025916, cmid loss: 0.069583
Epoch (89), Batch(1600/2189), loss: 0.095630, imid loss: 0.025837, cmid loss: 0.069794
Epoch (89), Batch(1800/2189), loss: 0.095986, imid loss: 0.025693, cmid loss: 0.070293
Epoch (89), Batch(2000/2189), loss: 0.096279, imid loss: 0.025790, cmid loss: 0.070489
Train 89, loss: 0.097048
Linear Accuracy : 0.8962722852512156
Start training epoch: (90/100)
Epoch (90), Batch(0/2189), loss: 0.043250, imid loss: 0.014436, cmid loss: 0.028814
Epoch (90), Batch(200/2189), loss: 0.108225, imid loss: 0.029600, cmid loss: 0.078625
Epoch (90), Batch(400/2189), loss: 0.106415, imid loss: 0.029778, cmid loss: 0.076637
Epoch (90), Batch(600/2189), loss: 0.099526, imid loss: 0.028110, cmid loss: 0.071416
Epoch (90), Batch(800/2189), loss: 0.100187, imid loss: 0.028109, cmid loss: 0.072078
Epoch (90), Batch(1000/2189), loss: 0.098845, imid loss: 0.027207, cmid loss: 0.071637
Epoch (90), Batch(1200/2189), loss: 0.097814, imid loss: 0.026708, cmid loss: 0.071106
Epoch (90), Batch(1400/2189), loss: 0.096221, imid loss: 0.026447, cmid loss: 0.069774
Epoch (90), Batch(1600/2189), loss: 0.095672, imid loss: 0.026555, cmid loss: 0.069117
Epoch (90), Batch(1800/2189), loss: 0.095322, imid loss: 0.026510, cmid loss: 0.068812
Epoch (90), Batch(2000/2189), loss: 0.095600, imid loss: 0.026772, cmid loss: 0.068828
Train 90, loss: 0.095871
Linear Accuracy : 0.8958670988654781
==> Saving...
Start training epoch: (91/100)
Epoch (91), Batch(0/2189), loss: 0.079991, imid loss: 0.020643, cmid loss: 0.059348
Epoch (91), Batch(200/2189), loss: 0.098237, imid loss: 0.027694, cmid loss: 0.070543
Epoch (91), Batch(400/2189), loss: 0.093418, imid loss: 0.026280, cmid loss: 0.067138
Epoch (91), Batch(600/2189), loss: 0.093210, imid loss: 0.025969, cmid loss: 0.067241
Epoch (91), Batch(800/2189), loss: 0.093899, imid loss: 0.026009, cmid loss: 0.067890
Epoch (91), Batch(1000/2189), loss: 0.096110, imid loss: 0.026710, cmid loss: 0.069399
Epoch (91), Batch(1200/2189), loss: 0.095140, imid loss: 0.026343, cmid loss: 0.068796
Epoch (91), Batch(1400/2189), loss: 0.094972, imid loss: 0.026353, cmid loss: 0.068619
Epoch (91), Batch(1600/2189), loss: 0.096164, imid loss: 0.026546, cmid loss: 0.069619
Epoch (91), Batch(1800/2189), loss: 0.095899, imid loss: 0.026648, cmid loss: 0.069252
Epoch (91), Batch(2000/2189), loss: 0.096460, imid loss: 0.026682, cmid loss: 0.069777
Train 91, loss: 0.096146
Linear Accuracy : 0.8970826580226904
Start training epoch: (92/100)
Epoch (92), Batch(0/2189), loss: 0.203539, imid loss: 0.054061, cmid loss: 0.149478
Epoch (92), Batch(200/2189), loss: 0.103076, imid loss: 0.027199, cmid loss: 0.075878
Epoch (92), Batch(400/2189), loss: 0.100943, imid loss: 0.027058, cmid loss: 0.073885
Epoch (92), Batch(600/2189), loss: 0.097567, imid loss: 0.026553, cmid loss: 0.071014
Epoch (92), Batch(800/2189), loss: 0.097943, imid loss: 0.026750, cmid loss: 0.071193
Epoch (92), Batch(1000/2189), loss: 0.098248, imid loss: 0.027199, cmid loss: 0.071050
Epoch (92), Batch(1200/2189), loss: 0.098871, imid loss: 0.027698, cmid loss: 0.071173
Epoch (92), Batch(1400/2189), loss: 0.097550, imid loss: 0.027358, cmid loss: 0.070192
Epoch (92), Batch(1600/2189), loss: 0.097972, imid loss: 0.027376, cmid loss: 0.070597
Epoch (92), Batch(1800/2189), loss: 0.096724, imid loss: 0.027258, cmid loss: 0.069466
Epoch (92), Batch(2000/2189), loss: 0.096986, imid loss: 0.027416, cmid loss: 0.069570
Train 92, loss: 0.096238
Linear Accuracy : 0.8914100486223663
Start training epoch: (93/100)
Epoch (93), Batch(0/2189), loss: 0.079876, imid loss: 0.011244, cmid loss: 0.068632
Epoch (93), Batch(200/2189), loss: 0.094598, imid loss: 0.028712, cmid loss: 0.065885
Epoch (93), Batch(400/2189), loss: 0.091789, imid loss: 0.027610, cmid loss: 0.064179
Epoch (93), Batch(600/2189), loss: 0.095246, imid loss: 0.029205, cmid loss: 0.066041
Epoch (93), Batch(800/2189), loss: 0.094358, imid loss: 0.028114, cmid loss: 0.066245
Epoch (93), Batch(1000/2189), loss: 0.093399, imid loss: 0.027376, cmid loss: 0.066023
Epoch (93), Batch(1200/2189), loss: 0.094003, imid loss: 0.027362, cmid loss: 0.066642
Epoch (93), Batch(1400/2189), loss: 0.094740, imid loss: 0.027670, cmid loss: 0.067070
Epoch (93), Batch(1600/2189), loss: 0.094685, imid loss: 0.027419, cmid loss: 0.067266
Epoch (93), Batch(1800/2189), loss: 0.094697, imid loss: 0.027298, cmid loss: 0.067399
Epoch (93), Batch(2000/2189), loss: 0.095392, imid loss: 0.027465, cmid loss: 0.067927
Train 93, loss: 0.096175
Linear Accuracy : 0.8982982171799028
Start training epoch: (94/100)
Epoch (94), Batch(0/2189), loss: 0.158586, imid loss: 0.044291, cmid loss: 0.114295
Epoch (94), Batch(200/2189), loss: 0.089695, imid loss: 0.026736, cmid loss: 0.062959
Epoch (94), Batch(400/2189), loss: 0.087900, imid loss: 0.025936, cmid loss: 0.061964
Epoch (94), Batch(600/2189), loss: 0.091175, imid loss: 0.025768, cmid loss: 0.065407
Epoch (94), Batch(800/2189), loss: 0.091703, imid loss: 0.025785, cmid loss: 0.065918
Epoch (94), Batch(1000/2189), loss: 0.093485, imid loss: 0.026382, cmid loss: 0.067103
Epoch (94), Batch(1200/2189), loss: 0.092967, imid loss: 0.026745, cmid loss: 0.066222
Epoch (94), Batch(1400/2189), loss: 0.093021, imid loss: 0.026650, cmid loss: 0.066371
Epoch (94), Batch(1600/2189), loss: 0.092556, imid loss: 0.026757, cmid loss: 0.065799
Epoch (94), Batch(1800/2189), loss: 0.092665, imid loss: 0.026567, cmid loss: 0.066098
Epoch (94), Batch(2000/2189), loss: 0.092869, imid loss: 0.026442, cmid loss: 0.066428
Train 94, loss: 0.093318
Linear Accuracy : 0.8962722852512156
Start training epoch: (95/100)
Epoch (95), Batch(0/2189), loss: 0.035510, imid loss: 0.013155, cmid loss: 0.022354
Epoch (95), Batch(200/2189), loss: 0.096822, imid loss: 0.028906, cmid loss: 0.067915
Epoch (95), Batch(400/2189), loss: 0.097610, imid loss: 0.027930, cmid loss: 0.069680
Epoch (95), Batch(600/2189), loss: 0.096930, imid loss: 0.028518, cmid loss: 0.068412
Epoch (95), Batch(800/2189), loss: 0.098733, imid loss: 0.028808, cmid loss: 0.069926
Epoch (95), Batch(1000/2189), loss: 0.098654, imid loss: 0.028240, cmid loss: 0.070414
Epoch (95), Batch(1200/2189), loss: 0.097406, imid loss: 0.027574, cmid loss: 0.069832
Epoch (95), Batch(1400/2189), loss: 0.097454, imid loss: 0.027635, cmid loss: 0.069819
Epoch (95), Batch(1600/2189), loss: 0.098412, imid loss: 0.027796, cmid loss: 0.070616
Epoch (95), Batch(1800/2189), loss: 0.098589, imid loss: 0.027907, cmid loss: 0.070682
Epoch (95), Batch(2000/2189), loss: 0.098062, imid loss: 0.027604, cmid loss: 0.070458
Train 95, loss: 0.097752
Linear Accuracy : 0.8938411669367909
==> Saving...
Start training epoch: (96/100)
Epoch (96), Batch(0/2189), loss: 0.124384, imid loss: 0.020497, cmid loss: 0.103887
Epoch (96), Batch(200/2189), loss: 0.099328, imid loss: 0.027556, cmid loss: 0.071771
Epoch (96), Batch(400/2189), loss: 0.098913, imid loss: 0.027069, cmid loss: 0.071844
Epoch (96), Batch(600/2189), loss: 0.096837, imid loss: 0.026824, cmid loss: 0.070013
Epoch (96), Batch(800/2189), loss: 0.097205, imid loss: 0.027139, cmid loss: 0.070066
Epoch (96), Batch(1000/2189), loss: 0.096022, imid loss: 0.026661, cmid loss: 0.069360
Epoch (96), Batch(1200/2189), loss: 0.096147, imid loss: 0.026784, cmid loss: 0.069364
Epoch (96), Batch(1400/2189), loss: 0.095942, imid loss: 0.026456, cmid loss: 0.069486
Epoch (96), Batch(1600/2189), loss: 0.096927, imid loss: 0.026848, cmid loss: 0.070079
Epoch (96), Batch(1800/2189), loss: 0.096799, imid loss: 0.026929, cmid loss: 0.069870
Epoch (96), Batch(2000/2189), loss: 0.096525, imid loss: 0.027026, cmid loss: 0.069499
Train 96, loss: 0.096111
Linear Accuracy : 0.8987034035656402
Start training epoch: (97/100)
Epoch (97), Batch(0/2189), loss: 0.046154, imid loss: 0.009842, cmid loss: 0.036312
Epoch (97), Batch(200/2189), loss: 0.100910, imid loss: 0.027849, cmid loss: 0.073061
Epoch (97), Batch(400/2189), loss: 0.093519, imid loss: 0.025289, cmid loss: 0.068230
Epoch (97), Batch(600/2189), loss: 0.096160, imid loss: 0.026999, cmid loss: 0.069161
Epoch (97), Batch(800/2189), loss: 0.095518, imid loss: 0.026865, cmid loss: 0.068653
Epoch (97), Batch(1000/2189), loss: 0.095487, imid loss: 0.026685, cmid loss: 0.068801
Epoch (97), Batch(1200/2189), loss: 0.096323, imid loss: 0.026866, cmid loss: 0.069457
Epoch (97), Batch(1400/2189), loss: 0.096210, imid loss: 0.027054, cmid loss: 0.069156
Epoch (97), Batch(1600/2189), loss: 0.096513, imid loss: 0.027191, cmid loss: 0.069323
Epoch (97), Batch(1800/2189), loss: 0.095577, imid loss: 0.026887, cmid loss: 0.068690
Epoch (97), Batch(2000/2189), loss: 0.095082, imid loss: 0.026896, cmid loss: 0.068186
Train 97, loss: 0.095818
Linear Accuracy : 0.9035656401944895
Start training epoch: (98/100)
Epoch (98), Batch(0/2189), loss: 0.166368, imid loss: 0.038317, cmid loss: 0.128051
Epoch (98), Batch(200/2189), loss: 0.101091, imid loss: 0.029195, cmid loss: 0.071896
Epoch (98), Batch(400/2189), loss: 0.095403, imid loss: 0.027586, cmid loss: 0.067817
Epoch (98), Batch(600/2189), loss: 0.094633, imid loss: 0.027290, cmid loss: 0.067343
Epoch (98), Batch(800/2189), loss: 0.096335, imid loss: 0.027540, cmid loss: 0.068795
Epoch (98), Batch(1000/2189), loss: 0.096109, imid loss: 0.027832, cmid loss: 0.068277
Epoch (98), Batch(1200/2189), loss: 0.096882, imid loss: 0.027938, cmid loss: 0.068943
Epoch (98), Batch(1400/2189), loss: 0.096814, imid loss: 0.027806, cmid loss: 0.069007
Epoch (98), Batch(1600/2189), loss: 0.096343, imid loss: 0.027642, cmid loss: 0.068702
Epoch (98), Batch(1800/2189), loss: 0.094918, imid loss: 0.027303, cmid loss: 0.067615
Epoch (98), Batch(2000/2189), loss: 0.094614, imid loss: 0.027163, cmid loss: 0.067451
Train 98, loss: 0.093959
Linear Accuracy : 0.8991085899513777
Start training epoch: (99/100)
Epoch (99), Batch(0/2189), loss: 0.269588, imid loss: 0.077266, cmid loss: 0.192322
Epoch (99), Batch(200/2189), loss: 0.094379, imid loss: 0.029190, cmid loss: 0.065189
Epoch (99), Batch(400/2189), loss: 0.097857, imid loss: 0.028848, cmid loss: 0.069009
Epoch (99), Batch(600/2189), loss: 0.096456, imid loss: 0.028012, cmid loss: 0.068444
Epoch (99), Batch(800/2189), loss: 0.095539, imid loss: 0.027934, cmid loss: 0.067605
Epoch (99), Batch(1000/2189), loss: 0.093518, imid loss: 0.027244, cmid loss: 0.066274
Epoch (99), Batch(1200/2189), loss: 0.093922, imid loss: 0.027168, cmid loss: 0.066754
Epoch (99), Batch(1400/2189), loss: 0.093722, imid loss: 0.026715, cmid loss: 0.067007
Epoch (99), Batch(1600/2189), loss: 0.094496, imid loss: 0.026812, cmid loss: 0.067684
Epoch (99), Batch(1800/2189), loss: 0.094718, imid loss: 0.026876, cmid loss: 0.067842
Epoch (99), Batch(2000/2189), loss: 0.094858, imid loss: 0.027003, cmid loss: 0.067855
Train 99, loss: 0.094680
Linear Accuracy : 0.893030794165316
==> Saving Last Model...