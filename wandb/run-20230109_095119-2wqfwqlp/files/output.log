Use Adam
Start training epoch: (0/100)
/home/zht/anaconda3/envs/py38torch19/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "train_crosspoint_update.py", line 438, in <module>
    train(args, io)
  File "train_crosspoint_update.py", line 228, in train
    _, point_feats = point_model(data)  # get features from inputs, the function f_theta in ori article
  File "/home/zht/anaconda3/envs/py38torch19/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/zht/github_play/crosspoint/CrossPoint/models/dgcnn.py", line 369, in forward
    x = get_graph_feature(x2, k=self.k)
  File "/home/zht/github_play/crosspoint/CrossPoint/models/dgcnn.py", line 27, in get_graph_feature
    idx = knn(x, k=k)  # (batch_size, num_points, k)
  File "/home/zht/github_play/crosspoint/CrossPoint/models/dgcnn.py", line 14, in knn
    inner = -2 * torch.matmul(x.transpose(2, 1), x)
RuntimeError: CUDA out of memory. Tried to allocate 640.00 MiB (GPU 1; 23.70 GiB total capacity; 2.96 GiB already allocated; 493.06 MiB free; 3.27 GiB reserved in total by PyTorch)